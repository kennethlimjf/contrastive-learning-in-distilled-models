% [1]
@misc{farahmand,
Author = {Megdad Farahmand},
Title = {Pre-trained Word Embeddings or Embedding Layer?},
Year = {2019},
url = {https://towardsdatascience.com/pre-trained-word-embeddings-or-embedding-layer-a-dilemma-8406959fd76c},
urldate = "2019 Jun 07",
note = {url: \href{https://towardsdatascience.com/pre-trained-word-embeddings-or-embedding-layer-a-dilemma-8406959fd76c}{link}},
}

% [2]
@misc{2105.11741,
Author = {Yuanmeng Yan and Rumei Li and Sirui Wang and Fuzheng Zhang and Wei Wu and Weiran Xu},
Title = {ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer},
Year = {2021},
Eprint = {\href{https://arxiv.org/abs/2105.11741}{arXiv:2105.11741}},
}

% [3]
@misc{2104.08821,
Author = {Tianyu Gao and Xingcheng Yao and Danqi Chen},
Title = {SimCSE: Simple Contrastive Learning of Sentence Embeddings},
Year = {2021},
Eprint = {\href{https://arxiv.org/abs/2104.08821}{arXiv:2104.08821}},
}

% [4]
@misc{1907.11692,
Author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
Title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
Year = {2019},
Eprint = {\href{https://arxiv.org/abs/1907.11692}{arXiv:1907.11692}},
}

% [5]
@article{10.3390/s22020450,
title = {Federated Learning in Edge Computing: A Systematic Survey},
volume = {22},
ISSN = {1424-8220},
url = {http://dx.doi.org/10.3390/s22020450},
note = {url: \url{http://dx.doi.org/10.3390/s22020450}},
DOI = {10.3390/s22020450},
number = {2},
journal = {Sensors},
publisher = {MDPI AG},
author = {Abreha, Haftay Gebreslasie and Hayajneh, Mohammad and Serhani, Mohamed Adel},
year = {2022},
month = {Jan},
pages = {450}
}

% [6]
@misc{1810.04805,
Author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
Title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
Year = {2018},
Eprint = {\href{https://arxiv.org/abs/1810.04805}{arXiv:1810.04805}},
}

% [7]
@misc{2009.14167,
Author = {Siqi Sun and Zhe Gan and Yu Cheng and Yuwei Fang and Shuohang Wang and Jingjing Liu},
Title = {Contrastive Distillation on Intermediate Representations for Language Model Compression},
Year = {2020},
Eprint = {\href{https://arxiv.org/abs/2009.14167}{arXiv:2009.14167}},
}

% [8]
@misc{2112.01642,
Author = {Shen Li and Jianqing Xu and Bryan Hooi},
Title = {Probabilistic Contrastive Loss for Self-Supervised Learning},
Year = {2021},
Eprint = {\href{https://arxiv.org/abs/2112.01642}{arXiv:2112.01642}},
}

% [9]
@misc{1910.01108,
Author = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
Title = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
Year = {2019},
Eprint = {\href{https://arxiv.org/abs/1910.01108}{arXiv:1910.01108}},
}

% [10]
@misc{2110.04403,
Author = {Oliver Zhang and Mike Wu and Jasmine Bayrooti and Noah Goodman},
Title = {Temperature as Uncertainty in Contrastive Learning},
Year = {2021},
Eprint = {\href{https://arxiv.org/abs/2110.04403}{arXiv:2110.04403}},
}

% [11]
@misc{2002.05709,
Author = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
Title = {A Simple Framework for Contrastive Learning of Visual Representations},
Year = {2020},
Eprint = {\href{https://arxiv.org/abs/2002.05709}{arXiv:2002.05709}},
}

% [12]
@misc{gatech,
Author = {Zsolt Kira},
Title  = {CS7643 Lecture 18: Unsupervised and Self-Supervised Learning},
Year   = {2022},
url    = {https://gatech.instructure.com/courses/236428/pages/unsupervised-and-self-supervised-learning?module_item_id=2104520},
note   = {url: \href{https://gatech.instructure.com/courses/236428/pages/unsupervised-and-self-supervised-learning?module_item_id=2104520}{Lecture 18}}
}

% [13]
@article{robust.sts,
  author = "Abhay Kashyap and Lushan Han and Jennifer Sleeman and Taneeya Satyapanich and Sunil Gandhi and Tim Finin",
  title = "Robust semantic text similarity using LSA, machine learning, and linguistic resources",
  journal = "Lang Resources \& Evaluation",
  volume = 50,
  year = 2016,
  DOI = {10.1007/s10579-015-9319-2},
  note = {url: \href{https://link.springer.com/article/10.1007/s10579-015-9319-2}{link}}
}

% [14]
@conference{reddragon,
  author    = "Yew Ken Chia, Sam Witteveen Martin Andrews",
  title     = "Transformer to CNN: Improved Text Classification for Edge Devices",
  booktitle = "32nd Conference on Neural Information Processing Systems",
  number    = 32,
  year      = 2018,
  note      = {url: \url{https://www.reddragon.ai/downloads/2018-12-NeurIPS_MLPCD2-workshop_Camera.pdf}},
}

% [15]
@online{cost.sota.ai,
Author = {Tony Peng, Michael Sarazen},
Title = {The Staggering Cost of Training SOTA AI Models},
Year = {2019},
url = {https://syncedreview.com/2019/06/27/the-staggering-cost-of-training-sota-ai-models/},
urldate = "2019 Jun 27",
note = {url: \href{https://syncedreview.com/2019/06/27/the-staggering-cost-of-training-sota-ai-models/}{link}}
}

% [16]
@misc{1803.05449,
Author = {Alexis Conneau and Douwe Kiela},
Title = {SentEval: An Evaluation Toolkit for Universal Sentence Representations},
Year = {2018},
Eprint = {\href{https://arxiv.org/abs/1803.05449}{arXiv:1803.05449}},
}

% [17]
@misc{1808.06670,
Author = {R Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Phil Bachman and Adam Trischler and Yoshua Bengio},
Title = {Learning deep representations by mutual information estimation and maximization},
Year = {2018},
Eprint = {\href{https://arxiv.org/abs/1808.06670}{arXiv:1808.06670}},
}

% [18]
@online{bert.as.service,
Author = {Han Xiao},
Title = {Why not the last hidden layer? Why second-to-last?},
Year = {2018},
url = {\url{https://bert-as-service.readthedocs.io/en/latest/section/faq.html?highlight=second-to-last#why-not-the-last-hidden-layer-why-second-to-last}},
note = {url: \href{https://bert-as-service.readthedocs.io/en/latest/section/faq.html}{Why not the last hidden layer}}
}

% [19]
@misc{https://doi.org/10.48550/arxiv.1908.10084,
  doi = {10.48550/ARXIV.1908.10084},
  Eprint = {\href{https://arxiv.org/abs/1908.10084}{arXiv:1908.10084}},
  url = {https://arxiv.org/abs/1908.10084},
  author = {Reimers, Nils and Gurevych, Iryna},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

% [20]
@inproceedings{li-etal-2020-sentence,
    title = "On the Sentence Embeddings from Pre-trained Language Models",
    author = "Li, Bohan  and
      Zhou, Hao  and
      He, Junxian  and
      Wang, Mingxuan  and
      Yang, Yiming  and
      Li, Lei",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.733",
    doi = "10.18653/v1/2020.emnlp-main.733",
    pages = "9119--9130",
    abstract = "Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at \url{https://github.com/bohanli/BERT-flow}.",
    note = {url: \url{https://aclanthology.org/2020.emnlp-main.733}}
}