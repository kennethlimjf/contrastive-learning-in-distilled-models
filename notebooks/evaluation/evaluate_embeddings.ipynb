{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T20:14:22.671634Z",
     "start_time": "2022-03-12T20:14:22.667939Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/gatech/cs7643-deep-learning/contrastive-learning-in-distilled-models\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T20:15:26.682185Z",
     "start_time": "2022-03-12T20:15:26.497008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/gatech/cs7643-deep-learning/contrastive-learning-in-distilled-models/notebooks/evaluation\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T20:14:24.085047Z",
     "start_time": "2022-03-12T20:14:23.339481Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T20:11:47.541382Z",
     "start_time": "2022-03-12T20:11:47.539431Z"
    }
   },
   "outputs": [],
   "source": [
    "# batch = [\n",
    "#     ['A man with a hard hat is dancing.', 'A man wearing a hard hat is dancing.'],\n",
    "#     ['A young child is riding a horse.', 'A child is riding a horse.'],\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [' '.join(s) for s in batch]\n",
    "# batch2 = tokenizer.batch_encode_plus(\n",
    "#     sentences,\n",
    "#     return_tensors='pt',\n",
    "#     padding=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentEval for BERT\n",
    "\n",
    "Use BERT Model outputs (first + last avg.) for evaluating SentEval STS Benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109482240"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(param.numel() for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s) for s in batch]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "    # Pooler\n",
    "    attention_mask = batch.attention_mask\n",
    "    last_hidden = outputs.last_hidden_state\n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "    first_hidden = hidden_states[0]\n",
    "    last_hidden = hidden_states[-1]\n",
    "    pooled_result = (\n",
    "        (first_hidden + last_hidden) / 2.0 * attention_mask.unsqueeze(-1)\n",
    "    ).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
    "\n",
    "    return pooled_result.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./data\"\n",
    "\n",
    "params = {\"task_path\": PATH_TO_DATA, \"usepytorch\": True, \"kfold\": 10}\n",
    "tasks = [\"STSBenchmark\"]\n",
    "\n",
    "se = senteval.engine.SE(params, batcher, prepare)\n",
    "results = se.eval(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.5619505330308854, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.5384285918923115, pvalue=0.0),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.6321576404364262, 3.5670578268133463e-168),\n",
       "   'spearman': SpearmanrResult(correlation=0.6371150398903414, pvalue=1.3235747770641853e-171),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.5327127002648916, 6.359296565156033e-102),\n",
       "   'spearman': SpearmanrResult(correlation=0.5386682573723041, pvalue=1.3268371385626441e-104),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.5731794732386623,\n",
       "    'mean': 0.5756069579107344,\n",
       "    'wmean': 0.5694831813530928},\n",
       "   'spearman': {'all': 0.5613396961884458,\n",
       "    'mean': 0.5714039630516523,\n",
       "    'wmean': 0.5556237901646753}}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5386682573723041"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stsb_spearman = results[\"STSBenchmark\"][\"test\"][\"spearman\"][0]\n",
    "stsb_spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentEval for DistilBERT\n",
    "\n",
    "Use DistilBERT model outputs (first + last avg.) for evaluating SentEval STS Benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed84f7e73d74f2db4e4a6cc770070e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73980563003e48c9b2b52547c767bd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66362880"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(param.numel() for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s) for s in batch]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "    # Pooler\n",
    "    attention_mask = batch.attention_mask\n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "    first_hidden = hidden_states[0]\n",
    "    last_hidden = hidden_states[-1]\n",
    "    pooled_result = (\n",
    "        (first_hidden + last_hidden) / 2.0 * attention_mask.unsqueeze(-1)\n",
    "    ).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
    "\n",
    "    return pooled_result.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./data\"\n",
    "\n",
    "params = {\"task_path\": PATH_TO_DATA, \"usepytorch\": True, \"kfold\": 10}\n",
    "tasks = [\"STSBenchmark\"]\n",
    "\n",
    "se = senteval.engine.SE(params, batcher, prepare)\n",
    "results = se.eval(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.6070898833059735, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.595530733737618, pvalue=0.0),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.6714695361612271, 3.181586253494327e-197),\n",
       "   'spearman': SpearmanrResult(correlation=0.684281683692436, pvalue=1.0932311093348739e-207),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.5632638275139215, 2.904861571468197e-116),\n",
       "   'spearman': SpearmanrResult(correlation=0.590522771009074, pvalue=2.1926696952431418e-130),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.6130842362054584,\n",
       "    'mean': 0.6139410823270407,\n",
       "    'wmean': 0.6112778003604056},\n",
       "   'spearman': {'all': 0.6146094901418286,\n",
       "    'mean': 0.6234450628130427,\n",
       "    'wmean': 0.6101598997470715}}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.590522771009074"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stsb_spearman = results[\"STSBenchmark\"][\"test\"][\"spearman\"][0]\n",
    "stsb_spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-text-base were not used when initializing Data2VecTextModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Data2VecTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Data2VecTextModel were not initialized from the model checkpoint at facebook/data2vec-text-base and are newly initialized: ['data2vec_text.pooler.dense.weight', 'data2vec_text.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import senteval\n",
    "\n",
    "from transformers import RobertaTokenizer, Data2VecTextModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Data2VecTextModel.from_pretrained(\"facebook/data2vec-text-base\").to(device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"facebook/data2vec-text-base\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124645632"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(param.numel() for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s) for s in batch]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "    # Pooler\n",
    "    attention_mask = batch.attention_mask\n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "    first_hidden = hidden_states[0]\n",
    "    last_hidden = hidden_states[-1]\n",
    "    pooled_result = (\n",
    "        (first_hidden + last_hidden) / 2.0 * attention_mask.unsqueeze(-1)\n",
    "    ).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
    "\n",
    "    return pooled_result.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./data\"\n",
    "\n",
    "params = {\"task_path\": PATH_TO_DATA, \"usepytorch\": True, \"kfold\": 10}\n",
    "tasks = [\"STSBenchmark\"]\n",
    "\n",
    "se = senteval.engine.SE(params, batcher, prepare)\n",
    "results = se.eval(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.4077605518815012,\n",
       "    3.019824514080955e-229),\n",
       "   'spearman': SpearmanrResult(correlation=0.43023015272142645, pvalue=8.902496901736272e-258),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.4987420550448654, 3.808025041749306e-95),\n",
       "   'spearman': SpearmanrResult(correlation=0.538374823495507, pvalue=1.7567365855034568e-113),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.38863728141271414, 6.03015552446043e-51),\n",
       "   'spearman': SpearmanrResult(correlation=0.4227786076610967, pvalue=6.670376017457212e-61),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.4201148641848045,\n",
       "    'mean': 0.43171329611302695,\n",
       "    'wmean': 0.42052147732987727},\n",
       "   'spearman': {'all': 0.45186988160019936,\n",
       "    'mean': 0.4637945279593434,\n",
       "    'wmean': 0.4478404129813855}}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4227786076610967"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stsb_spearman = results[\"STSBenchmark\"][\"test\"][\"spearman\"][0]\n",
    "stsb_spearman"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "cl-distilled",
   "language": "python",
   "name": "cl-distilled"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.438px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}