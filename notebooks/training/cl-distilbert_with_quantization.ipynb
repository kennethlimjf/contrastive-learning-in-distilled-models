{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Learning From Scratch - DistilBERT\n",
    "\n",
    "An attempt to build contrastive learning model from scratch. Parts include:\n",
    "\n",
    "- Loading and preparing Wiki-1M data for model input\n",
    "- Contrastive learning model\n",
    "  - Forward passing using pre-trained model\n",
    "  - Constrastive layer\n",
    "  - Calculate loss\n",
    "- Training procedure\n",
    "  - Default trainer optimizer\n",
    "  - Default trainer hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set Project home\n",
    "PROJECT_HOME = os.path.join('/',\n",
    "                            'Users',\n",
    "                            'ng-ka',\n",
    "                            'OMSCS',\n",
    "                            'DL',\n",
    "                            'DLProject',\n",
    "                            'contrastive-learning-in-distilled-models')\n",
    "%cd {PROJECT_HOME}\n",
    "\n",
    "# Load project code\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "#import distilface\n",
    "import src.distilface as distilface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ng-ka\\\\OMSCS\\\\DL\\\\DLProject\\\\contrastive-learning-in-distilled-models'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertCLModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertCLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertCLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertCLModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertCLModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertModel, DistilBertPreTrainedModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPooling\n",
    "\n",
    "from src.distilface.modules.pooler import Pooler\n",
    "from src.distilface.modules.similarity import Similarity\n",
    "\n",
    "class DistilBertCLModel(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, pooler_type='avg_last4', temp=0.05):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.pooler_type = pooler_type\n",
    "        self.temp = 0.05\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pooler = Pooler(pooler_type)\n",
    "        self.sim = Similarity(temp=temp)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        if self.training:\n",
    "            return self.cl_forward(self.distilbert, input_ids, attention_mask)\n",
    "        else:\n",
    "            return self.sent_emb(self.distilbert, input_ids, attention_mask)\n",
    "\n",
    "    def cl_forward(self, encoder, input_ids=None, attention_mask=None):\n",
    "        batch_size = input_ids.size(0)#64#input_ids.size(0)\n",
    "        num_sent = input_ids.size(1)  # Number of sentences in one instance: 2 sentences\n",
    "\n",
    "        # Flatten all input tensors\n",
    "        input_ids = input_ids.view((-1, input_ids.size(-1))) # (bs * num_sent, len)\n",
    "        attention_mask = attention_mask.view((-1, attention_mask.size(-1))) # (bs * num_sent len)\n",
    "\n",
    "        # Pre-trained Model Encoder\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # Pooling\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "        pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-1)))  # (bs, num_sent, hidden)\n",
    "\n",
    "        # Separate representation\n",
    "        z1, z2 = pooler_output[:, 0], pooler_output[:, 1]\n",
    "\n",
    "        # Cosine similarity\n",
    "        cos_sim = self.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "\n",
    "        # Calculate contrastive loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(self.device)\n",
    "        loss = criterion(cos_sim, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cos_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def sent_emb(self, encoder, input_ids=None, attention_mask=None):\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            pooler_output=pooler_output,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )\n",
    "\n",
    "\n",
    "pretrained_model_name = 'distilbert-base-uncased'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "model = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "#model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertCLModel(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): Pooler()\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('batch128_model.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model =torch.quantization.quantize_dynamic(model, qconfig_spec={nn.Linear},  dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 265489.337\n",
      "model:  int8  \t Size (KB): 138116.329\n",
      "1.92 times smaller\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "# compare the sizes\n",
    "f=print_size_of_model(model,\"fp32\")\n",
    "q=print_size_of_model(quantized_model,\"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with default BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Preparing Wiki-1M data\n",
    "\n",
    "Use huggingface `datasets` library to load local file data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-84caea1147087fa9\n",
      "Reusing dataset text (C:\\Users\\ng-ka\\.cache\\huggingface\\datasets\\text\\default-84caea1147087fa9\\0.0.0\\4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {'train': 'data/training/wiki1m_for_simcse.txt'}\n",
    "datasets = load_dataset('text', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_names: ['text']\n",
      "sent0_cname: text | sent1_cname: text\n"
     ]
    }
   ],
   "source": [
    "# Unsupervised / Self-supervised dataset\n",
    "\n",
    "column_names = datasets[\"train\"].column_names\n",
    "sent0_cname = column_names[0]\n",
    "sent1_cname = column_names[0]\n",
    "\n",
    "print('column_names:', column_names)\n",
    "print('sent0_cname:', sent0_cname, '| sent1_cname:', sent1_cname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(examples):\n",
    "    total = len(examples[sent0_cname])\n",
    "\n",
    "    # Avoid \"None\" fields \n",
    "    for idx in range(total):\n",
    "        if examples[sent0_cname][idx] is None:\n",
    "            examples[sent0_cname][idx] = \" \"\n",
    "        if examples[sent1_cname][idx] is None:\n",
    "            examples[sent1_cname][idx] = \" \"\n",
    "    \n",
    "    sentences = examples[sent0_cname] + examples[sent1_cname]\n",
    "\n",
    "    sent_features = tokenizer(\n",
    "        sentences,\n",
    "        max_length=32,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    features = {}\n",
    "    for key in sent_features:\n",
    "        features[key] = [[sent_features[key][i], sent_features[key][i+total]] for i in range(total)]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\ng-ka\\.cache\\huggingface\\datasets\\text\\default-84caea1147087fa9\\0.0.0\\4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\\cache-dd04d9dec26aeca9.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets[\"train\"].map(prepare_features,\n",
    "                                      batched=True,\n",
    "                                    #   num_proc=24,\n",
    "                                      remove_columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence 1 and Sentence 2 are the same sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Contrastive Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertCLModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertCLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertCLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, BertModel, BertPreTrainedModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPooling\n",
    "\n",
    "from distilface.modules.pooler import Pooler\n",
    "from distilface.modules.similarity import Similarity\n",
    "\n",
    "\n",
    "class BertCLModel(BertPreTrainedModel):\n",
    "    def __init__(self, config, pooler_type='avg_first_last', temp=0.05):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.pooler_type = pooler_type\n",
    "        self.temp = 0.05\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.pooler = Pooler(pooler_type)\n",
    "        self.sim = Similarity(temp=temp)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
    "        if self.training:\n",
    "            return self.cl_forward(self.bert, input_ids, attention_mask, token_type_ids)\n",
    "        else:\n",
    "            return self.sent_emb(self.bert, input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "    def cl_forward(self, encoder, input_ids=None, attention_mask=None, token_type_ids=None):\n",
    "        batch_size = input_ids.size(0)\n",
    "        num_sent = input_ids.size(1)  # Number of sentences in one instance: 2 sentences\n",
    "\n",
    "        # Flatten all input tensors\n",
    "        input_ids = input_ids.view((-1, input_ids.size(-1))) # (bs * num_sent, len)\n",
    "        attention_mask = attention_mask.view((-1, attention_mask.size(-1))) # (bs * num_sent len)\n",
    "        token_type_ids = token_type_ids.view((-1, token_type_ids.size(-1))) # (bs * num_sent, len)\n",
    "\n",
    "        # Pre-trained Model Encoder\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # Pooling\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "        pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-1)))  # (bs, num_sent, hidden)\n",
    "\n",
    "        # Separate representation\n",
    "        z1, z2 = pooler_output[:, 0], pooler_output[:, 1]\n",
    "\n",
    "        # Cosine similarity\n",
    "        cos_sim = self.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "\n",
    "        # Calculate contrastive loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(self.device)\n",
    "        loss = criterion(cos_sim, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cos_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def sent_emb(self, encoder, input_ids=None, attention_mask=None, token_type_ids=None):\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            pooler_output=pooler_output,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )\n",
    "\n",
    "\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "model = BertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initial BERT embeddings performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import senteval\n",
    "\n",
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s) for s in batch]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    pooled_result = outputs.pooler_output.cpu()\n",
    "\n",
    "    return pooled_result\n",
    "\n",
    "\n",
    "def evaluate_model():\n",
    "    PATH_TO_DATA = \"./data\"\n",
    "\n",
    "    params = {\"task_path\": PATH_TO_DATA, \"usepytorch\": True, \"kfold\": 10}\n",
    "    tasks = [\"STSBenchmark\", 'STS12', 'STS13', 'STS14', 'STS15']\n",
    "\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "    #se = se_engine.SE(params, batcher, prepare)\n",
    "    results = se.eval(tasks)\n",
    "\n",
    "    print('STS12: ', results[\"STS12\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS13: ', results[\"STS13\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS14: ', results[\"STS14\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS15: ', results[\"STS15\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STSB: ', results[\"STSBenchmark\"][\"test\"][\"spearman\"][0])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=1e-05,\n",
    "    per_device_train_batch_size= 128,\n",
    "    per_device_eval_batch_size = 128,\n",
    "    weight_decay=0.0,\n",
    "    num_train_epochs=2,\n",
    "    max_steps= 30000,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ng-ka\\\\OMSCS\\\\DL\\\\DLProject\\\\contrastive-learning-in-distilled-models'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\anaconda3\\envs\\cl-distilled\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000000\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30000' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30000/30000 3:51:18, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output\\checkpoint-5000\n",
      "Configuration saved in output\\checkpoint-5000\\config.json\n",
      "Model weights saved in output\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-10000\n",
      "Configuration saved in output\\checkpoint-10000\\config.json\n",
      "Model weights saved in output\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-15000\n",
      "Configuration saved in output\\checkpoint-15000\\config.json\n",
      "Model weights saved in output\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-15000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-20000\n",
      "Configuration saved in output\\checkpoint-20000\\config.json\n",
      "Model weights saved in output\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-20000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-25000\n",
      "Configuration saved in output\\checkpoint-25000\\config.json\n",
      "Model weights saved in output\\checkpoint-25000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-25000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-25000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-30000\n",
      "Configuration saved in output\\checkpoint-30000\\config.json\n",
      "Model weights saved in output\\checkpoint-30000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-30000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-30000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'trained_model/bert_cl'\n",
    "\n",
    "train_result = trainer.train()\n",
    "torch.save(model, './bert_model_best_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 435650.293\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "# compare the sizes\n",
    "f=print_size_of_model(model,\"fp32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 435650.293\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "# compare the sizes\n",
    "f=print_size_of_model(model,\"fp32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=print_size_of_model(quantized_model,\"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate DistilBert CL Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS12:  0.6522348679699254\n",
      "STS13:  0.7245264269994489\n",
      "STS14:  0.6591385536542372\n",
      "STS15:  0.7803083813296277\n",
      "STSB:  0.7317242200449551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.7701978269277896, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.7444988754489594, pvalue=0.0),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.792601430214866, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.7923150663580373, pvalue=0.0),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.7395838046909169, 4.979487826512024e-239),\n",
       "   'spearman': SpearmanrResult(correlation=0.7317242200449551, pvalue=1.7326856147338482e-231),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.7683647566030786,\n",
       "    'mean': 0.7674610206111909,\n",
       "    'wmean': 0.7691997588084071},\n",
       "   'spearman': {'all': 0.7548919636159135,\n",
       "    'mean': 0.7561793872839839,\n",
       "    'wmean': 0.7507700897004076}}},\n",
       " 'STS12': {'MSRpar': {'pearson': (0.6587949262407169, 1.5689202234706753e-94),\n",
       "   'spearman': SpearmanrResult(correlation=0.6366967124028455, pvalue=1.678438485809478e-86),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (0.8536988445568228, 3.8250578501088855e-214),\n",
       "   'spearman': SpearmanrResult(correlation=0.8484792708623435, pvalue=6.624907739378852e-209),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.5164816904486852, 1.1635179273872324e-32),\n",
       "   'spearman': SpearmanrResult(correlation=0.5952382035750191, pvalue=2.43260390535798e-45),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.7138032874990049, 8.029507875907565e-118),\n",
       "   'spearman': SpearmanrResult(correlation=0.6947446645580526, pvalue=3.7278633554301037e-109),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.6581016211793618, 7.143974085692276e-51),\n",
       "   'spearman': SpearmanrResult(correlation=0.5879248830777675, pvalue=1.8555478590687155e-38),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.7069207633714811,\n",
       "    'mean': 0.6801760739849183,\n",
       "    'wmean': 0.697995635935946},\n",
       "   'spearman': {'all': 0.6522348679699254,\n",
       "    'mean': 0.6726167468952057,\n",
       "    'wmean': 0.6894262708032157}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.5403151553255987, 1.0207250701755277e-15),\n",
       "   'spearman': SpearmanrResult(correlation=0.5848838782020366, pvalue=9.832741978366368e-19),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (0.7313093681559517, 1.9625236627075765e-126),\n",
       "   'spearman': SpearmanrResult(correlation=0.7173859181606141, pvalue=1.568992515213928e-119),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.7345926321227985, 3.167469276742905e-96),\n",
       "   'spearman': SpearmanrResult(correlation=0.7342747263141658, pvalue=4.206869681517781e-96),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': 0.7171945633150137,\n",
       "    'mean': 0.6687390518681163,\n",
       "    'wmean': 0.7084720380629278},\n",
       "   'spearman': {'all': 0.7245264269994489,\n",
       "    'mean': 0.6788481742256055,\n",
       "    'wmean': 0.7070070753752618}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (0.5215587622662345,\n",
       "    9.343138531670588e-33),\n",
       "   'spearman': SpearmanrResult(correlation=0.508523934763729, pvalue=5.744448186293206e-31),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (0.8013870476948712, 1.7717330852474213e-68),\n",
       "   'spearman': SpearmanrResult(correlation=0.7709496848235667, pvalue=2.4484395981516876e-60),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (0.7451721200687449, 9.46220917969318e-134),\n",
       "   'spearman': SpearmanrResult(correlation=0.7212894362502648, pvalue=2.0088375511523874e-121),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.7749328096219927, 3.4197289017445545e-151),\n",
       "   'spearman': SpearmanrResult(correlation=0.7440169915774423, pvalue=4.015389089369636e-133),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.7994813884766737, 9.832759062944258e-168),\n",
       "   'spearman': SpearmanrResult(correlation=0.8123071843031526, pvalue=2.528974415180825e-177),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (0.7000792322868198, 1.6406231191391438e-111),\n",
       "   'spearman': SpearmanrResult(correlation=0.6527722606870168, pvalue=2.8233458924619958e-92),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.6834652955428022,\n",
       "    'mean': 0.7237685600692227,\n",
       "    'wmean': 0.7306311253783841},\n",
       "   'spearman': {'all': 0.6591385536542372,\n",
       "    'mean': 0.7016432487341953,\n",
       "    'wmean': 0.7087760215211081}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.7143622843810458,\n",
       "    8.521334360334148e-60),\n",
       "   'spearman': SpearmanrResult(correlation=0.717130123341652, pvalue=1.865638104815194e-60),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.7390487700610201,\n",
       "    1.8428341692904094e-130),\n",
       "   'spearman': SpearmanrResult(correlation=0.7404577040278972, pvalue=3.287398896408933e-131),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.768297587208495, 2.871357227274023e-74),\n",
       "   'spearman': SpearmanrResult(correlation=0.7826415004393404, pvalue=8.479720632987105e-79),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.7876011189010238, 1.8720917648392475e-159),\n",
       "   'spearman': SpearmanrResult(correlation=0.788291434898209, pvalue=6.395881563291198e-160),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.8437452641992345, 2.5360315476033285e-204),\n",
       "   'spearman': SpearmanrResult(correlation=0.850240206668032, pvalue=1.1918244530676417e-210),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.7787279451354893,\n",
       "    'mean': 0.7706110049501638,\n",
       "    'wmean': 0.7779312722390123},\n",
       "   'spearman': {'all': 0.7803083813296277,\n",
       "    'mean': 0.7757521938750261,\n",
       "    'wmean': 0.7822187893711586}}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "results = evaluate_model()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 64 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertCLModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertCLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertCLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertCLModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertCLModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertModel, DistilBertPreTrainedModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPooling\n",
    "\n",
    "from src.distilface.modules.pooler import Pooler\n",
    "from src.distilface.modules.similarity import Similarity\n",
    "\n",
    "from torch.cuda.amp import autocast \n",
    "\n",
    "#scaler = GradScaler()\n",
    "\n",
    "class DistilBertCLModel(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, pooler_type='avg_first_last', temp=0.05):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.pooler_type = pooler_type\n",
    "        self.temp = 0.05\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pooler = Pooler(pooler_type)\n",
    "        self.sim = Similarity(temp=temp)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        with autocast():\n",
    "            if self.training:\n",
    "                return self.cl_forward(self.distilbert, input_ids, attention_mask)\n",
    "            else:\n",
    "                return self.sent_emb(self.distilbert, input_ids, attention_mask)\n",
    "\n",
    "    def cl_forward(self, encoder, input_ids=None, attention_mask=None):\n",
    "        batch_size = input_ids.size(0)#64#input_ids.size(0)\n",
    "        num_sent = input_ids.size(1)  # Number of sentences in one instance: 2 sentences\n",
    "\n",
    "        # Flatten all input tensors\n",
    "        input_ids = input_ids.view((-1, input_ids.size(-1))) # (bs * num_sent, len)\n",
    "        attention_mask = attention_mask.view((-1, attention_mask.size(-1))) # (bs * num_sent len)\n",
    "\n",
    "        # Pre-trained Model Encoder\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # Pooling\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "        pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-1)))  # (bs, num_sent, hidden)\n",
    "\n",
    "        # Separate representation\n",
    "        z1, z2 = pooler_output[:, 0], pooler_output[:, 1]\n",
    "\n",
    "        # Cosine similarity\n",
    "        cos_sim = self.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "\n",
    "        # Calculate contrastive loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(self.device)\n",
    "        loss = criterion(cos_sim, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cos_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def sent_emb(self, encoder, input_ids=None, attention_mask=None):\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            pooler_output=pooler_output,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )\n",
    "\n",
    "\n",
    "pretrained_model_name = 'distilbert-base-uncased'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "model2 = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "#model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "#model2 = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=False,\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size= 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    weight_decay=0.0,\n",
    "    num_train_epochs=2,\n",
    "    max_steps= 30000,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "model2.train()\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1000000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30000' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30000/30000 49:40, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output\\checkpoint-5000\n",
      "Configuration saved in output\\checkpoint-5000\\config.json\n",
      "Model weights saved in output\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-10000\n",
      "Configuration saved in output\\checkpoint-10000\\config.json\n",
      "Model weights saved in output\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-15000\n",
      "Configuration saved in output\\checkpoint-15000\\config.json\n",
      "Model weights saved in output\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-15000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-20000\n",
      "Configuration saved in output\\checkpoint-20000\\config.json\n",
      "Model weights saved in output\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-20000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-25000\n",
      "Configuration saved in output\\checkpoint-25000\\config.json\n",
      "Model weights saved in output\\checkpoint-25000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-25000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-25000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-30000\n",
      "Configuration saved in output\\checkpoint-30000\\config.json\n",
      "Model weights saved in output\\checkpoint-30000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-30000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-30000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'trained_model/distilbert_cl'\n",
    "\n",
    "#train_result = trainer.train(model_path=model_path)\n",
    "train_result2 = trainer2.train()\n",
    "torch.save(model2, './batch64_fp16_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import senteval\n",
    "#import SentEval.senteval as senteval\n",
    "#import SentEval_simcse.senteval as senteval\n",
    "#import SentEval_simcse.senteval.engine as se_engine\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s) for s in batch]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model2(**batch)\n",
    "\n",
    "    pooled_result = outputs.pooler_output.cpu()\n",
    "\n",
    "    return pooled_result\n",
    "\n",
    "\n",
    "def evaluate_model():\n",
    "    PATH_TO_DATA = \"./data\"\n",
    "\n",
    "    params = {\"task_path\": PATH_TO_DATA, \"usepytorch\": True, \"kfold\": 10}\n",
    "    tasks = [\"STSBenchmark\", 'STS12', 'STS13', 'STS14', 'STS15']\n",
    "\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "    #se = se_engine.SE(params, batcher, prepare)\n",
    "    results = se.eval(tasks)\n",
    "\n",
    "    print('STS12: ', results[\"STS12\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS13: ', results[\"STS13\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS14: ', results[\"STS14\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS15: ', results[\"STS15\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STSB: ', results[\"STSBenchmark\"][\"test\"][\"spearman\"][0])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS12:  0.5860086657732572\n",
      "STS13:  0.7462740979049128\n",
      "STS14:  0.6726454938423136\n",
      "STS15:  0.7632500735789172\n",
      "STSB:  0.7239425725655111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.7514974091222588, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.728916173975978, pvalue=0.0),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.7515928663596871, 5.587693491694605e-273),\n",
       "   'spearman': SpearmanrResult(correlation=0.7571062733767996, pvalue=2.91924659407765e-279),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.7324146663602896, 3.863341707867918e-232),\n",
       "   'spearman': SpearmanrResult(correlation=0.7239425725655111, pvalue=2.7955081361595227e-224),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.745320014694186,\n",
       "    'mean': 0.7451683139474118,\n",
       "    'wmean': 0.7484640391161609},\n",
       "   'spearman': {'all': 0.7376444193730962,\n",
       "    'mean': 0.7366550066394296,\n",
       "    'wmean': 0.7330221722091952}}},\n",
       " 'STS12': {'MSRpar': {'pearson': (0.6420318665513999, 2.2231580917506402e-88),\n",
       "   'spearman': SpearmanrResult(correlation=0.6250375288907182, pvalue=1.5936298376149268e-82),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (0.7821232084995746, 8.181160183934045e-156),\n",
       "   'spearman': SpearmanrResult(correlation=0.7790254725438126, pvalue=8.414656636378357e-154),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.45579831148351924, 6.297898948763149e-25),\n",
       "   'spearman': SpearmanrResult(correlation=0.5411141409400582, pvalue=2.857397403925653e-36),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.7136179540663932, 9.826163343218386e-118),\n",
       "   'spearman': SpearmanrResult(correlation=0.6934438014827883, pvalue=1.3748744170757464e-108),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.6382798657374623, 4.883208877814331e-47),\n",
       "   'spearman': SpearmanrResult(correlation=0.5870457672237382, pvalue=2.541504852813737e-38),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.6183244598202621,\n",
       "    'mean': 0.6463702412676698,\n",
       "    'wmean': 0.6651270473739409},\n",
       "   'spearman': {'all': 0.5860086657732572,\n",
       "    'mean': 0.6451333422162231,\n",
       "    'wmean': 0.6614326750327374}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.5921135950867219, 2.8765211843152323e-19),\n",
       "   'spearman': SpearmanrResult(correlation=0.6319923990253394, pvalue=1.8117240840180167e-22),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (0.7731684011000663, 4.38333085981378e-150),\n",
       "   'spearman': SpearmanrResult(correlation=0.7648096228034668, pvalue=5.712060967612397e-145),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.7505444160034295, 1.1982965729735423e-102),\n",
       "   'spearman': SpearmanrResult(correlation=0.7529617459710697, pvalue=1.155236306238404e-103),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': 0.7356590985615045,\n",
       "    'mean': 0.7052754707300726,\n",
       "    'wmean': 0.7418941251162428},\n",
       "   'spearman': {'all': 0.7462740979049128,\n",
       "    'mean': 0.7165879225999586,\n",
       "    'wmean': 0.7436435466721062}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (0.49831797558248253,\n",
       "    1.2792776975535832e-29),\n",
       "   'spearman': SpearmanrResult(correlation=0.4926043889193346, pvalue=6.948272302777438e-29),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (0.8110025260607239, 2.380008735032673e-71),\n",
       "   'spearman': SpearmanrResult(correlation=0.7784025416510902, pvalue=3.2806411805675245e-62),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (0.7329794941714723, 2.726975432764373e-127),\n",
       "   'spearman': SpearmanrResult(correlation=0.7083285273142585, pvalue=2.9246656405827755e-115),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.7772277704984392, 1.1963720155832323e-152),\n",
       "   'spearman': SpearmanrResult(correlation=0.7520869211811828, pvalue=1.3993776204836034e-137),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.8194292780522218, 5.661217058142552e-183),\n",
       "   'spearman': SpearmanrResult(correlation=0.8233160109197186, pvalue=3.656972868344196e-186),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (0.7580125092137587, 5.774364069468944e-141),\n",
       "   'spearman': SpearmanrResult(correlation=0.693617134014335, pvalue=1.155885093917403e-108),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.7000196305767966,\n",
       "    'mean': 0.7328282589298497,\n",
       "    'wmean': 0.7422081695419342},\n",
       "   'spearman': {'all': 0.6726454938423136,\n",
       "    'mean': 0.7080592539999867,\n",
       "    'wmean': 0.7168544486883064}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.7173434681197128,\n",
       "    1.6582532080464822e-60),\n",
       "   'spearman': SpearmanrResult(correlation=0.7142110057220189, pvalue=9.25415062761234e-60),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.7474115712934761, 5.613920515876082e-135),\n",
       "   'spearman': SpearmanrResult(correlation=0.7473711672805158, pvalue=5.908993578060504e-135),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.7646068271362548, 3.723372015412307e-73),\n",
       "   'spearman': SpearmanrResult(correlation=0.7678182729123182, pvalue=4.015857954976702e-74),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.7832781266505436, 1.4260279924234083e-156),\n",
       "   'spearman': SpearmanrResult(correlation=0.781320748444706, pvalue=2.7367543960174458e-155),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.8425177008665632, 3.6954389140605657e-203),\n",
       "   'spearman': SpearmanrResult(correlation=0.8518734860030123, pvalue=2.7387336828358623e-212),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.7603051391751539,\n",
       "    'mean': 0.77103153881331,\n",
       "    'wmean': 0.7785456366096417},\n",
       "   'spearman': {'all': 0.7632500735789172,\n",
       "    'mean': 0.7725189360725142,\n",
       "    'wmean': 0.7803950102613506}}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.eval()\n",
    "\n",
    "results2 = evaluate_model()\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 256 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertCLModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertCLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertCLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import default_data_collator\n",
    "\n",
    "model3 = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "\n",
    "training_args3 = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size= 256,\n",
    "    per_device_eval_batch_size = 256,\n",
    "    weight_decay=0.0,\n",
    "    num_train_epochs=2,\n",
    "    max_steps= 30000,\n",
    "    logging_steps=10000,\n",
    "    save_steps=10000\n",
    ")\n",
    "\n",
    "model3.train()\n",
    "\n",
    "trainer3 = Trainer(\n",
    "    model=model3,\n",
    "    args=training_args3,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\anaconda3\\envs\\cl-distilled\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000000\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30000' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30000/30000 2:16:18, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output\\checkpoint-10000\n",
      "Configuration saved in output\\checkpoint-10000\\config.json\n",
      "Model weights saved in output\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-20000\n",
      "Configuration saved in output\\checkpoint-20000\\config.json\n",
      "Model weights saved in output\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-20000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-30000\n",
      "Configuration saved in output\\checkpoint-30000\\config.json\n",
      "Model weights saved in output\\checkpoint-30000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-30000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-30000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'trained_model/distilbert_cl'\n",
    "\n",
    "#train_result = trainer.train(model_path=model_path)\n",
    "train_result3 = trainer3.train()\n",
    "torch.save(model3, './batch256_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS12:  0.5697409578579856\n",
      "STS13:  0.6769343320004665\n",
      "STS14:  0.6357083664225791\n",
      "STS15:  0.744575188106335\n",
      "STSB:  0.6320978112448171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.7108974767715875, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.6957934629797473, pvalue=0.0),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.7092411378218643, 1.0445253840454627e-229),\n",
       "   'spearman': SpearmanrResult(correlation=0.7145470998781457, pvalue=1.0790202728598763e-234),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.6362564654216417, 2.357894834089188e-157),\n",
       "   'spearman': SpearmanrResult(correlation=0.6320978112448171, pvalue=1.0318637044184543e-154),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.694542147265105,\n",
       "    'mean': 0.6854650266716978,\n",
       "    'wmean': 0.6986797596788475},\n",
       "   'spearman': {'all': 0.692993221621729,\n",
       "    'mean': 0.68081279136757,\n",
       "    'wmean': 0.6888734527346301}}},\n",
       " 'STS12': {'MSRpar': {'pearson': (0.5685692938261935, 1.922991733409069e-65),\n",
       "   'spearman': SpearmanrResult(correlation=0.5768180019887325, pvalue=9.865649661539039e-68),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (0.7231459307017963, 2.4625344140537457e-122),\n",
       "   'spearman': SpearmanrResult(correlation=0.7286285308858846, pvalue=4.5212690570561215e-125),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.4552691576823324, 7.245333920427569e-25),\n",
       "   'spearman': SpearmanrResult(correlation=0.5652776900635411, pvalue=4.102512606437534e-40),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.7052620559338071, 7.496786418423562e-114),\n",
       "   'spearman': SpearmanrResult(correlation=0.6820584737385141, pvalue=9.392905669563112e-104),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.5305233033717558, 2.4040708126567676e-30),\n",
       "   'spearman': SpearmanrResult(correlation=0.5054581332408665, pvalue=2.897420355990552e-27),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.5730881336602889,\n",
       "    'mean': 0.5965539483031771,\n",
       "    'wmean': 0.6172394793332912},\n",
       "   'spearman': {'all': 0.5697409578579856,\n",
       "    'mean': 0.6116481659835078,\n",
       "    'wmean': 0.6279823069054438}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.4756524584171261, 4.624072307242327e-12),\n",
       "   'spearman': SpearmanrResult(correlation=0.5113077740400039, pvalue=5.556470480427763e-14),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (0.7621181361889477, 2.283711136237502e-143),\n",
       "   'spearman': SpearmanrResult(correlation=0.7555961434929108, pvalue=1.4234421879289685e-139),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.6084402620525589, 4.166232111198533e-58),\n",
       "   'spearman': SpearmanrResult(correlation=0.6193585571805185, pvalue=1.0011027739565939e-60),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': 0.6575950672170406,\n",
       "    'mean': 0.615403618886211,\n",
       "    'wmean': 0.6685479358626888},\n",
       "   'spearman': {'all': 0.6769343320004665,\n",
       "    'mean': 0.628754158237811,\n",
       "    'wmean': 0.67386295166101}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (0.49407384596966303,\n",
       "    4.510113001940756e-29),\n",
       "   'spearman': SpearmanrResult(correlation=0.49240417290292743, pvalue=7.368507905860281e-29),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (0.7780622258261892, 4.009255437301985e-62),\n",
       "   'spearman': SpearmanrResult(correlation=0.7427848525503551, pvalue=7.446349832307482e-54),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (0.7432213991850618, 1.0817147703149346e-132),\n",
       "   'spearman': SpearmanrResult(correlation=0.7196824151471459, pvalue=1.219101168718968e-120),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.7054349774996785, 6.2505242558663144e-114),\n",
       "   'spearman': SpearmanrResult(correlation=0.7030675288710679, pvalue=7.44650639954171e-113),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.7165954891406879, 3.758118604101963e-119),\n",
       "   'spearman': SpearmanrResult(correlation=0.7432297325332733, pvalue=1.070564915797356e-132),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (0.7485302385446341, 1.3539932639329752e-135),\n",
       "   'spearman': SpearmanrResult(correlation=0.6983425494584553, pvalue=9.724897786576716e-111),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.6494945972063338,\n",
       "    'mean': 0.6976530293609858,\n",
       "    'wmean': 0.7042902604564671},\n",
       "   'spearman': {'all': 0.6357083664225791,\n",
       "    'mean': 0.6832518752438709,\n",
       "    'wmean': 0.6913757341543681}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.6607010288385147,\n",
       "    2.1493530919525173e-48),\n",
       "   'spearman': SpearmanrResult(correlation=0.6458298799190522, pvalue=1.2242984122795279e-45),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.7441626662553017,\n",
       "    3.3477261803153333e-133),\n",
       "   'spearman': SpearmanrResult(correlation=0.7497939271081, pvalue=2.691221448723987e-136),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.6857797494730589, 2.045705688150149e-53),\n",
       "   'spearman': SpearmanrResult(correlation=0.6781616879833227, pvalue=7.745591607805545e-52),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.7823121289240607, 6.152161449073835e-156),\n",
       "   'spearman': SpearmanrResult(correlation=0.7836294107948686, pvalue=8.364444785613939e-157),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.7792083240587948, 6.414589359548605e-154),\n",
       "   'spearman': SpearmanrResult(correlation=0.7855348556994918, pvalue=4.550816591458857e-158),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.7431130125811777,\n",
       "    'mean': 0.7304327795099461,\n",
       "    'wmean': 0.7447308770984861},\n",
       "   'spearman': {'all': 0.744575188106335,\n",
       "    'mean': 0.7285899523009671,\n",
       "    'wmean': 0.7452384943884118}}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.eval()\n",
    "\n",
    "results3 = evaluate_model()\n",
    "results3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertCLModel(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, pooler_type='avg_first_last', temp=0.05):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.pooler_type = pooler_type\n",
    "        self.temp = 0.05\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pooler = Pooler(pooler_type)\n",
    "        self.sim = Similarity(temp=temp)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        if self.training:\n",
    "            return self.cl_forward(self.distilbert, input_ids, attention_mask)\n",
    "        else:\n",
    "            return self.sent_emb(self.distilbert, input_ids, attention_mask)\n",
    "\n",
    "    def cl_forward(self, encoder, input_ids=None, attention_mask=None):\n",
    "        batch_size = input_ids.size(0)#64#input_ids.size(0)\n",
    "        num_sent = input_ids.size(1)  # Number of sentences in one instance: 2 sentences\n",
    "\n",
    "        # Flatten all input tensors\n",
    "        input_ids = input_ids.view((-1, input_ids.size(-1))) # (bs * num_sent, len)\n",
    "        attention_mask = attention_mask.view((-1, attention_mask.size(-1))) # (bs * num_sent len)\n",
    "\n",
    "        # Pre-trained Model Encoder\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # Pooling\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "        pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-1)))  # (bs, num_sent, hidden)\n",
    "\n",
    "        # Separate representation\n",
    "        z1, z2 = pooler_output[:, 0], pooler_output[:, 1]\n",
    "\n",
    "        # Cosine similarity\n",
    "        cos_sim = self.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "\n",
    "        # Calculate contrastive loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(self.device)\n",
    "        loss = criterion(cos_sim, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cos_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def sent_emb(self, encoder, input_ids=None, attention_mask=None):\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            pooler_output=pooler_output,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertCLModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertCLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertCLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import default_data_collator\n",
    "\n",
    "model4 = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "\n",
    "training_args4 = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size= 256,\n",
    "    per_device_eval_batch_size = 256,\n",
    "    weight_decay=0.0,\n",
    "    num_train_epochs=2,\n",
    "    max_steps= 30000,\n",
    "    logging_steps=10000,\n",
    "    save_steps=10000,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "model4.train()\n",
    "\n",
    "trainer4 = Trainer(\n",
    "    model=model4,\n",
    "    args=training_args4,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\anaconda3\\envs\\cl-distilled\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000000\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30000' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30000/30000 2:20:12, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output\\checkpoint-10000\n",
      "Configuration saved in output\\checkpoint-10000\\config.json\n",
      "Model weights saved in output\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-20000\n",
      "Configuration saved in output\\checkpoint-20000\\config.json\n",
      "Model weights saved in output\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-20000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-30000\n",
      "Configuration saved in output\\checkpoint-30000\\config.json\n",
      "Model weights saved in output\\checkpoint-30000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-30000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-30000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'trained_model/distilbert_cl'\n",
    "\n",
    "#train_result = trainer.train(model_path=model_path)\n",
    "train_result4 = trainer4.train()\n",
    "torch.save(model4, './batch256_16bit_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import senteval\n",
    "#import SentEval.senteval as senteval\n",
    "#import SentEval_simcse.senteval as senteval\n",
    "#import SentEval_simcse.senteval.engine as se_engine\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s) for s in batch]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model4(**batch)\n",
    "\n",
    "    pooled_result = outputs.pooler_output.cpu()\n",
    "\n",
    "    return pooled_result\n",
    "\n",
    "\n",
    "def evaluate_model():\n",
    "    PATH_TO_DATA = \"./data\"\n",
    "\n",
    "    params = {\"task_path\": PATH_TO_DATA, \"usepytorch\": True, \"kfold\": 10}\n",
    "    tasks = [\"STSBenchmark\", 'STS12', 'STS13', 'STS14', 'STS15']\n",
    "\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "    #se = se_engine.SE(params, batcher, prepare)\n",
    "    results = se.eval(tasks)\n",
    "\n",
    "    print('STS12: ', results[\"STS12\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS13: ', results[\"STS13\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS14: ', results[\"STS14\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS15: ', results[\"STS15\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STSB: ', results[\"STSBenchmark\"][\"test\"][\"spearman\"][0])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS12:  0.6028650839372993\n",
      "STS13:  0.740729745204717\n",
      "STS14:  0.692465335997525\n",
      "STS15:  0.779843190057093\n",
      "STSB:  0.7226870749431175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.7654012355643011, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.7482016388305586, pvalue=0.0),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.7670311528467801, 5.261345910632041e-291),\n",
       "   'spearman': SpearmanrResult(correlation=0.7708464868002052, pvalue=1.119990676071557e-295),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.7318742172557201, 1.2511244745112543e-231),\n",
       "   'spearman': SpearmanrResult(correlation=0.7226870749431175, pvalue=3.856394414212571e-223),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.7581791623069682,\n",
       "    'mean': 0.7547688685556003,\n",
       "    'wmean': 0.7603260289899136},\n",
       "   'spearman': {'all': 0.7518720772834617,\n",
       "    'mean': 0.7472450668579604,\n",
       "    'wmean': 0.7480605503226412}}},\n",
       " 'STS12': {'MSRpar': {'pearson': (0.6638885051937329, 1.767074037152879e-96),\n",
       "   'spearman': SpearmanrResult(correlation=0.6412098115217912, pvalue=4.352737200439051e-88),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (0.7927758724538693, 5.40449629193965e-163),\n",
       "   'spearman': SpearmanrResult(correlation=0.7968791798219567, pvalue=7.134120069574861e-166),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.4883130499588775, 7.050370165247265e-29),\n",
       "   'spearman': SpearmanrResult(correlation=0.5617743042099863, pvalue=1.5484094953816606e-39),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.7033376326266234, 5.619884938784154e-113),\n",
       "   'spearman': SpearmanrResult(correlation=0.6892578269390826, pvalue=8.745419703218659e-107),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.6080656023385231, 1.0486671075968402e-41),\n",
       "   'spearman': SpearmanrResult(correlation=0.5443363044978058, pvalue=3.7156818879919706e-32),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.6248317529346296,\n",
       "    'mean': 0.6512761325143253,\n",
       "    'wmean': 0.6714142126672666},\n",
       "   'spearman': {'all': 0.6028650839372993,\n",
       "    'mean': 0.6466914853981246,\n",
       "    'wmean': 0.6662016424836329}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.5146130120631479, 3.5912269769391216e-14),\n",
       "   'spearman': SpearmanrResult(correlation=0.5541006568636835, pvalue=1.3296955257355115e-16),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (0.791466568163673, 4.343420463492872e-162),\n",
       "   'spearman': SpearmanrResult(correlation=0.7867191383722396, pvalue=7.340512667313412e-159),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.718557980499374, 3.1875352751854185e-90),\n",
       "   'spearman': SpearmanrResult(correlation=0.7196172523078505, pvalue=1.3179279906224866e-90),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': 0.7300588889331164,\n",
       "    'mean': 0.6748791869087315,\n",
       "    'wmean': 0.7293152083085589},\n",
       "   'spearman': {'all': 0.740729745204717,\n",
       "    'mean': 0.6868123491812579,\n",
       "    'wmean': 0.7323131043140799}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (0.5482619939866078,\n",
       "    1.1340073971181462e-36),\n",
       "   'spearman': SpearmanrResult(correlation=0.5465012098040761, pvalue=2.1073801020387983e-36),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (0.8040898056033967, 2.866409535608092e-69),\n",
       "   'spearman': SpearmanrResult(correlation=0.7820627545980403, pvalue=3.7087254650744173e-63),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (0.7720103540905534, 2.309219404096038e-149),\n",
       "   'spearman': SpearmanrResult(correlation=0.7484062908114091, pvalue=1.585670027031293e-135),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.7894573369989675, 1.0331236218204271e-160),\n",
       "   'spearman': SpearmanrResult(correlation=0.7658776866620939, pvalue=1.3036430142850932e-145),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.7827745917618757, 3.0583123984807816e-156),\n",
       "   'spearman': SpearmanrResult(correlation=0.7960594594391224, pvalue=2.7154627127692566e-165),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (0.7839778031262034, 4.922939526615252e-157),\n",
       "   'spearman': SpearmanrResult(correlation=0.7282291668892134, pvalue=7.191616695300215e-125),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.7165183287279627,\n",
       "    'mean': 0.7467619809279341,\n",
       "    'wmean': 0.7557626409221846},\n",
       "   'spearman': {'all': 0.692465335997525,\n",
       "    'mean': 0.7278560947006593,\n",
       "    'wmean': 0.7358596863047}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.7090791257127155,\n",
       "    1.4727280952748954e-58),\n",
       "   'spearman': SpearmanrResult(correlation=0.7072934280830891, pvalue=3.803429223181467e-58),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.7434218437198311, 8.430054312425235e-133),\n",
       "   'spearman': SpearmanrResult(correlation=0.7452786342918362, pvalue=8.278240687049306e-134),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.767196130871143, 6.199544565738762e-74),\n",
       "   'spearman': SpearmanrResult(correlation=0.7719957694989369, pvalue=2.0991322486796266e-75),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.8187385387418046, 2.0499519323950233e-182),\n",
       "   'spearman': SpearmanrResult(correlation=0.8194708023401971, pvalue=5.238884421992033e-183),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.8488625315593523, 2.7751885771480814e-209),\n",
       "   'spearman': SpearmanrResult(correlation=0.8526581459634256, pvalue=4.3980824199909614e-213),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.7781699965647223,\n",
       "    'mean': 0.7774596341209694,\n",
       "    'wmean': 0.7872901355782294},\n",
       "   'spearman': {'all': 0.779843190057093,\n",
       "    'mean': 0.7793393560354971,\n",
       "    'wmean': 0.7892630453466182}}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.eval()\n",
    "\n",
    "results4 = evaluate_model()\n",
    "results4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl-distilled",
   "language": "python",
   "name": "cl-distilled"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
