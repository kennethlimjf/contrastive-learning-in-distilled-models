{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Learning From Scratch - DistilBERT\n",
    "\n",
    "An attempt to build contrastive learning model from scratch. Parts include:\n",
    "\n",
    "- Loading and preparing Wiki-1M data for model input\n",
    "- Contrastive learning model\n",
    "  - Forward passing using pre-trained model\n",
    "  - Constrastive layer\n",
    "  - Calculate loss\n",
    "- Training procedure\n",
    "  - Default trainer optimizer\n",
    "  - Default trainer hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set Project home\n",
    "PROJECT_HOME = os.path.join('/',\n",
    "                            'Users',\n",
    "                            'ng-ka',\n",
    "                            'OMSCS',\n",
    "                            'DL',\n",
    "                            'DLProject',\n",
    "                            'contrastive-learning-in-distilled-models')\n",
    "%cd {PROJECT_HOME}\n",
    "\n",
    "# Load project code\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "#import distilface\n",
    "import src.distilface as distilface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Preparing Wiki-1M data\n",
    "\n",
    "Use huggingface `datasets` library to load local file data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\anaconda3\\envs\\cl-distilled\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration default-84caea1147087fa9\n",
      "Reusing dataset text (C:\\Users\\ng-ka\\.cache\\huggingface\\datasets\\text\\default-84caea1147087fa9\\0.0.0\\4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 66.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {'train': 'data/training/wiki1m_for_simcse.txt'}\n",
    "# data_files = {'train': 'data/training/wiki5k.txt'}\n",
    "datasets = load_dataset('text', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_names: ['text']\n",
      "sent0_cname: text | sent1_cname: text\n"
     ]
    }
   ],
   "source": [
    "# Unsupervised / Self-supervised dataset\n",
    "\n",
    "column_names = datasets[\"train\"].column_names\n",
    "sent0_cname = column_names[0]\n",
    "sent1_cname = column_names[0]\n",
    "\n",
    "print('column_names:', column_names)\n",
    "print('sent0_cname:', sent0_cname, '| sent1_cname:', sent1_cname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(examples):\n",
    "    total = len(examples[sent0_cname])\n",
    "\n",
    "    # Avoid \"None\" fields \n",
    "    for idx in range(total):\n",
    "        if examples[sent0_cname][idx] is None:\n",
    "            examples[sent0_cname][idx] = \" \"\n",
    "        if examples[sent1_cname][idx] is None:\n",
    "            examples[sent1_cname][idx] = \" \"\n",
    "    \n",
    "    sentences = examples[sent0_cname] + examples[sent1_cname]\n",
    "\n",
    "    sent_features = tokenizer(\n",
    "        sentences,\n",
    "        max_length=32,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    features = {}\n",
    "    for key in sent_features:\n",
    "        features[key] = [[sent_features[key][i], sent_features[key][i+total]] for i in range(total)]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\ng-ka\\.cache\\huggingface\\datasets\\text\\default-84caea1147087fa9\\0.0.0\\4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\\cache-1293cb42b0393e2c.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets[\"train\"].map(prepare_features,\n",
    "                                      batched=True,\n",
    "                                    #   num_proc=24,\n",
    "                                      remove_columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[101, 26866, 1999, 2148, 2660, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(train_dataset['input_ids'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[101, 26866, 1999, 2148, 2660, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(train_dataset['input_ids'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attention_mask', 'input_ids'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence 1 and Sentence 2 are the same sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Contrastive Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertCLModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertCLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertCLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertCLModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertCLModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertModel, DistilBertPreTrainedModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPooling\n",
    "\n",
    "from src.distilface.modules.pooler import Pooler\n",
    "from src.distilface.modules.similarity import Similarity\n",
    "\n",
    "#from torch.cuda.amp import autocast \n",
    "\n",
    "#scaler = GradScaler()\n",
    "\n",
    "class DistilBertCLModel(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, pooler_type='avg_first_last', temp=0.001):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.pooler_type = pooler_type\n",
    "        self.temp = 0.001\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pooler = Pooler(pooler_type)\n",
    "        self.sim = Similarity(temp=temp)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        if self.training:\n",
    "            return self.cl_forward(self.distilbert, input_ids, attention_mask)\n",
    "        else:\n",
    "            return self.sent_emb(self.distilbert, input_ids, attention_mask)\n",
    "\n",
    "    def cl_forward(self, encoder, input_ids=None, attention_mask=None):\n",
    "        batch_size = input_ids.size(0)#64#input_ids.size(0)\n",
    "        num_sent = input_ids.size(1)  # Number of sentences in one instance: 2 sentences\n",
    "\n",
    "        # Flatten all input tensors\n",
    "        input_ids = input_ids.view((-1, input_ids.size(-1))) # (bs * num_sent, len)\n",
    "        attention_mask = attention_mask.view((-1, attention_mask.size(-1))) # (bs * num_sent len)\n",
    "\n",
    "        # Pre-trained Model Encoder\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # Pooling\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "        pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-1)))  # (bs, num_sent, hidden)\n",
    "\n",
    "        # Separate representation\n",
    "        z1, z2 = pooler_output[:, 0], pooler_output[:, 1]\n",
    "\n",
    "        # Cosine similarity\n",
    "        cos_sim = self.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "\n",
    "        # Calculate contrastive loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(self.device)\n",
    "        loss = criterion(cos_sim, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cos_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def sent_emb(self, encoder, input_ids=None, attention_mask=None):\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            pooler_output=pooler_output,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )\n",
    "\n",
    "\n",
    "pretrained_model_name = 'distilbert-base-uncased'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "model = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "#model.eval();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initial DistilBERT embeddings performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import senteval\n",
    "#import SentEval.senteval as senteval\n",
    "#import SentEval_simcse.senteval as senteval\n",
    "#import SentEval_simcse.senteval.engine as se_engine\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s) for s in batch]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    pooled_result = outputs.pooler_output.cpu()\n",
    "\n",
    "    return pooled_result\n",
    "\n",
    "\n",
    "def evaluate_model():\n",
    "    PATH_TO_DATA = \"./data\"\n",
    "\n",
    "    params = {\"task_path\": PATH_TO_DATA, \"usepytorch\": True, \"kfold\": 10}\n",
    "    tasks = [\"STSBenchmark\", 'STS12', 'STS13', 'STS14', 'STS15']\n",
    "\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "    #se = se_engine.SE(params, batcher, prepare)\n",
    "    results = se.eval(tasks)\n",
    "\n",
    "    print('STS12: ', results[\"STS12\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS13: ', results[\"STS13\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS14: ', results[\"STS14\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS15: ', results[\"STS15\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STSB: ', results[\"STSBenchmark\"][\"test\"][\"spearman\"][0])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS12:  0.47596608898755144\n",
      "STS13:  0.618674507613396\n",
      "STS14:  0.5294342415078909\n",
      "STS15:  0.6969415778069489\n",
      "STSB:  0.5905314299135291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.607072923019217, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.5955188195020807, pvalue=0.0),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.6714570536745258, 3.2552287443463876e-197),\n",
       "   'spearman': SpearmanrResult(correlation=0.6842681833563105, pvalue=1.122076247507796e-207),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.5632499403360769, 2.9511246107835275e-116),\n",
       "   'spearman': SpearmanrResult(correlation=0.5905314299135291, pvalue=2.169060496705334e-130),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.6130684398138361,\n",
       "    'mean': 0.6139266390099399,\n",
       "    'wmean': 0.6112621097209918},\n",
       "   'spearman': {'all': 0.6145998749710233,\n",
       "    'mean': 0.6234394775906401,\n",
       "    'wmean': 0.6101509979372606}}},\n",
       " 'STS12': {'MSRpar': {'pearson': (0.42118590394249555, 1.3145639639932755e-33),\n",
       "   'spearman': SpearmanrResult(correlation=0.45991561737043646, pvalue=1.5821923016995031e-40),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (0.6352879593428346, 5.183287142463953e-86),\n",
       "   'spearman': SpearmanrResult(correlation=0.6401519760834063, pvalue=1.0302269231217165e-87),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.5022933949818664, 1.0349314233022616e-30),\n",
       "   'spearman': SpearmanrResult(correlation=0.5933517188464215, pvalue=5.388261366699939e-45),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.6877233214631762, 3.937747104046696e-106),\n",
       "   'spearman': SpearmanrResult(correlation=0.6910408351722502, pvalue=1.5043383888872297e-107),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.6239686926226475, 1.936629867889363e-44),\n",
       "   'spearman': SpearmanrResult(correlation=0.55171310954646, pvalue=3.69868564914955e-33),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.45145305382748163,\n",
       "    'mean': 0.5740918544706041,\n",
       "    'wmean': 0.5751814881642513},\n",
       "   'spearman': {'all': 0.47596608898755144,\n",
       "    'mean': 0.5872346514037948,\n",
       "    'wmean': 0.5906735170943098}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.4721083831978025, 6.9827543985655566e-12),\n",
       "   'spearman': SpearmanrResult(correlation=0.5056664404698936, pvalue=1.1578223671604005e-13),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (0.7104445114930775, 3.042310153878186e-116),\n",
       "   'spearman': SpearmanrResult(correlation=0.6922823812457632, pvalue=4.382855824474567e-108),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.5347802083410838, 8.09701529699335e-43),\n",
       "   'spearman': SpearmanrResult(correlation=0.5696084647773406, pvalue=1.4436226629736743e-49),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': 0.6078202925938808,\n",
       "    'mean': 0.5724443676773213,\n",
       "    'wmean': 0.6147157099490271},\n",
       "   'spearman': {'all': 0.618674507613396,\n",
       "    'mean': 0.5891857621643325,\n",
       "    'wmean': 0.6228887279488137}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (0.3917755391727156,\n",
       "    5.8568246329610094e-18),\n",
       "   'spearman': SpearmanrResult(correlation=0.4072090543164891, pvalue=2.1076338923720368e-19),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (0.7777888751111236, 4.708872222201854e-62),\n",
       "   'spearman': SpearmanrResult(correlation=0.7388030216217484, pvalue=5.251752312600481e-53),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (0.6712030793623832, 2.4054316831219454e-99),\n",
       "   'spearman': SpearmanrResult(correlation=0.6258466508423443, pvalue=8.546905534030859e-83),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.6677905891921765, 5.351525206116833e-98),\n",
       "   'spearman': SpearmanrResult(correlation=0.6484275025055973, pvalue=1.111432521900915e-90),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.6718954026358572, 1.2754177080826265e-99),\n",
       "   'spearman': SpearmanrResult(correlation=0.712151158241655, pvalue=4.830318594690074e-117),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (0.673753528437716, 2.3036170201828703e-100),\n",
       "   'spearman': SpearmanrResult(correlation=0.6339111659638288, pvalue=1.5514556778231942e-85),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.5359667255865653,\n",
       "    'mean': 0.6423678356519954,\n",
       "    'wmean': 0.6461646946352424},\n",
       "   'spearman': {'all': 0.5294342415078909,\n",
       "    'mean': 0.6277247589152771,\n",
       "    'wmean': 0.6320366237584036}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.6212710339076668,\n",
       "    2.0750726869911188e-41),\n",
       "   'spearman': SpearmanrResult(correlation=0.6092295308452247, pvalue=1.8060620383169866e-39),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.7166850875969126, 3.4043557410727e-119),\n",
       "   'spearman': SpearmanrResult(correlation=0.7213544950173122, pvalue=1.8669199180594134e-121),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.6882026219663078, 6.290663012093747e-54),\n",
       "   'spearman': SpearmanrResult(correlation=0.7037745246672049, pvalue=2.416171437304839e-57),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.7268860584257288, 3.4047667656413547e-124),\n",
       "   'spearman': SpearmanrResult(correlation=0.714097476476157, pvalue=5.825624158024163e-118),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.7687635296327311, 2.3132115123040793e-147),\n",
       "   'spearman': SpearmanrResult(correlation=0.7762308473471929, pvalue=5.158601346804e-152),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.6851086541146186,\n",
       "    'mean': 0.7043616663058694,\n",
       "    'wmean': 0.71676787589809},\n",
       "   'spearman': {'all': 0.6969415778069489,\n",
       "    'mean': 0.7049373748706184,\n",
       "    'wmean': 0.7170462116492193}}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_model()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size= 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    weight_decay=0.0,\n",
    "    num_train_epochs=2,\n",
    "    max_steps= 20000,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ng-ka\\\\OMSCS\\\\DL\\\\DLProject\\\\contrastive-learning-in-distilled-models'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\anaconda3\\envs\\cl-distilled\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20000/20000 35:25, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output\\checkpoint-5000\n",
      "Configuration saved in output\\checkpoint-5000\\config.json\n",
      "Model weights saved in output\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-10000\n",
      "Configuration saved in output\\checkpoint-10000\\config.json\n",
      "Model weights saved in output\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-15000\n",
      "Configuration saved in output\\checkpoint-15000\\config.json\n",
      "Model weights saved in output\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-15000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-20000\n",
      "Configuration saved in output\\checkpoint-20000\\config.json\n",
      "Model weights saved in output\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-20000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'trained_model/distilbert_cl'\n",
    "\n",
    "#train_result = trainer.train(model_path=model_path)\n",
    "train_result = trainer.train()\n",
    "torch.save(model, './0_001temp_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate DistilBert CL Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS12:  0.44181732871886\n",
      "STS13:  0.5550227020734463\n",
      "STS14:  0.4927035606920931\n",
      "STS15:  0.6338052684486225\n",
      "STSB:  0.5381977129616206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.5633161573597276, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.555619143532377, pvalue=0.0),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.6278288747780895, 3.140816697763277e-165),\n",
       "   'spearman': SpearmanrResult(correlation=0.6500256591318583, pvalue=7.663657392517889e-181),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.5100715292980883, 3.311410990404457e-92),\n",
       "   'spearman': SpearmanrResult(correlation=0.5381977129616206, pvalue=2.170536918669308e-104),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.5651793669725098,\n",
       "    'mean': 0.5670721871453018,\n",
       "    'wmean': 0.5660218520781493},\n",
       "   'spearman': {'all': 0.5716460142352061,\n",
       "    'mean': 0.5812808385419519,\n",
       "    'wmean': 0.569247518664754}}},\n",
       " 'STS12': {'MSRpar': {'pearson': (0.3636008874177728, 7.385776535127104e-25),\n",
       "   'spearman': SpearmanrResult(correlation=0.4087784845408527, pvalue=1.4199592757653228e-31),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (0.5955179687482708, 3.5846823077523784e-73),\n",
       "   'spearman': SpearmanrResult(correlation=0.6034443181665743, pvalue=1.3775772642404321e-75),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.4676562282841708, 2.5531435280913066e-26),\n",
       "   'spearman': SpearmanrResult(correlation=0.5875034346339117, pvalue=6.13489083795974e-44),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.6589337085910144, 1.3900158454744465e-94),\n",
       "   'spearman': SpearmanrResult(correlation=0.6738766581131264, pvalue=2.0557320412270956e-100),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.5718640520260226, 5.013106376729087e-36),\n",
       "   'spearman': SpearmanrResult(correlation=0.47646204681217097, pvalue=5.277744350101207e-24),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.42848193580153426,\n",
       "    'mean': 0.5315145690134503,\n",
       "    'wmean': 0.532936740382436},\n",
       "   'spearman': {'all': 0.44181732871886,\n",
       "    'mean': 0.5500129884533271,\n",
       "    'wmean': 0.5548092113225344}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.29925698475117946, 2.875888384779067e-05),\n",
       "   'spearman': SpearmanrResult(correlation=0.3067699223062188, pvalue=1.7596612645144884e-05),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (0.6653859191277671, 4.6467404877210334e-97),\n",
       "   'spearman': SpearmanrResult(correlation=0.6514341934088047, pvalue=8.80675441373333e-92),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.46381426862403097, 2.8566639447723853e-31),\n",
       "   'spearman': SpearmanrResult(correlation=0.5010514513000223, pvalue=5.432855829527869e-37),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': 0.5545121094242741,\n",
       "    'mean': 0.47615239083432587,\n",
       "    'wmean': 0.5438658761079197},\n",
       "   'spearman': {'all': 0.5550227020734463,\n",
       "    'mean': 0.4864185223383486,\n",
       "    'wmean': 0.5517633497011942}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (0.32893826112705105,\n",
       "    8.150697328551531e-13),\n",
       "   'spearman': SpearmanrResult(correlation=0.37953577067523514, pvalue=7.244101363935096e-17),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (0.7095764434420854, 3.192909716145983e-47),\n",
       "   'spearman': SpearmanrResult(correlation=0.6811227485200336, pvalue=3.0465380754954755e-42),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (0.6409160114702239, 5.531352606884285e-88),\n",
       "   'spearman': SpearmanrResult(correlation=0.6077491200049558, pvalue=6.290455603668846e-77),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.6072899380379204, 8.76277854615995e-77),\n",
       "   'spearman': SpearmanrResult(correlation=0.6088882311999606, pvalue=2.7576372978959384e-77),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.5925779521312421, 2.7123754010683673e-72),\n",
       "   'spearman': SpearmanrResult(correlation=0.6500252761315011, pvalue=2.899529234867025e-91),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (0.5728875825203678, 1.239690093108097e-66),\n",
       "   'spearman': SpearmanrResult(correlation=0.5728049191069601, pvalue=1.3069882348830415e-66),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.4827230216202271,\n",
       "    'mean': 0.5753643647881485,\n",
       "    'wmean': 0.5789730036425639},\n",
       "   'spearman': {'all': 0.4927035606920931,\n",
       "    'mean': 0.5833543442731077,\n",
       "    'wmean': 0.5879276216513065}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.499429054555364,\n",
       "    4.72529241199218e-25),\n",
       "   'spearman': SpearmanrResult(correlation=0.48398942759304897, pvalue=2.04138930138789e-23),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.7144998796415781, 3.753640911757856e-118),\n",
       "   'spearman': SpearmanrResult(correlation=0.7247757236632907, pvalue=3.84558906047851e-123),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.5327258552845895, 7.090440422919085e-29),\n",
       "   'spearman': SpearmanrResult(correlation=0.5557622695497889, pvalue=8.831706074649855e-32),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.6892054697511031, 9.207550674505216e-107),\n",
       "   'spearman': SpearmanrResult(correlation=0.6855556562619025, pvalue=3.2467251758447207e-105),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.7136767291856939, 9.216810645329028e-118),\n",
       "   'spearman': SpearmanrResult(correlation=0.7277853276229516, pvalue=1.2034240593984099e-124),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.6126042408087292,\n",
       "    'mean': 0.6299073976836658,\n",
       "    'wmean': 0.658364883374588},\n",
       "   'spearman': {'all': 0.6338052684486225,\n",
       "    'mean': 0.6355736809381964,\n",
       "    'wmean': 0.664498139029891}}}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "results = evaluate_model()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# temp 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertCLModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertCLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertCLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertCLModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertCLModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from transformers import AutoTokenizer, DistilBertModel, DistilBertPreTrainedModel, AutoConfig\n",
    "# from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPooling\n",
    "\n",
    "# from src.distilface.modules.pooler import Pooler\n",
    "# from src.distilface.modules.similarity import Similarity\n",
    "\n",
    "#from torch.cuda.amp import autocast \n",
    "\n",
    "#scaler = GradScaler()\n",
    "\n",
    "class DistilBertCLModel(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, pooler_type='avg_first_last', temp=0.01):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.pooler_type = pooler_type\n",
    "        self.temp = 0.01\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pooler = Pooler(pooler_type)\n",
    "        self.sim = Similarity(temp=temp)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        if self.training:\n",
    "            return self.cl_forward(self.distilbert, input_ids, attention_mask)\n",
    "        else:\n",
    "            return self.sent_emb(self.distilbert, input_ids, attention_mask)\n",
    "\n",
    "    def cl_forward(self, encoder, input_ids=None, attention_mask=None):\n",
    "        batch_size = input_ids.size(0)#64#input_ids.size(0)\n",
    "        num_sent = input_ids.size(1)  # Number of sentences in one instance: 2 sentences\n",
    "\n",
    "        # Flatten all input tensors\n",
    "        input_ids = input_ids.view((-1, input_ids.size(-1))) # (bs * num_sent, len)\n",
    "        attention_mask = attention_mask.view((-1, attention_mask.size(-1))) # (bs * num_sent len)\n",
    "\n",
    "        # Pre-trained Model Encoder\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # Pooling\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "        pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-1)))  # (bs, num_sent, hidden)\n",
    "\n",
    "        # Separate representation\n",
    "        z1, z2 = pooler_output[:, 0], pooler_output[:, 1]\n",
    "\n",
    "        # Cosine similarity\n",
    "        cos_sim = self.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "\n",
    "        # Calculate contrastive loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(self.device)\n",
    "        loss = criterion(cos_sim, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cos_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def sent_emb(self, encoder, input_ids=None, attention_mask=None):\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            pooler_output=pooler_output,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )\n",
    "\n",
    "\n",
    "pretrained_model_name = 'distilbert-base-uncased'\n",
    "\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model2 = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args2 = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=False,\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size= 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    weight_decay=0.0,\n",
    "    num_train_epochs=2,\n",
    "    max_steps= 20000,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "model2.train()\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\anaconda3\\envs\\cl-distilled\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20000/20000 31:54, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output\\checkpoint-5000\n",
      "Configuration saved in output\\checkpoint-5000\\config.json\n",
      "Model weights saved in output\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-10000\n",
      "Configuration saved in output\\checkpoint-10000\\config.json\n",
      "Model weights saved in output\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-15000\n",
      "Configuration saved in output\\checkpoint-15000\\config.json\n",
      "Model weights saved in output\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-15000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-20000\n",
      "Configuration saved in output\\checkpoint-20000\\config.json\n",
      "Model weights saved in output\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-20000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'trained_model/distilbert_cl'\n",
    "\n",
    "#train_result = trainer.train(model_path=model_path)\n",
    "train_result2 = trainer2.train()\n",
    "torch.save(model2, './0_01temp_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import senteval\n",
    "#import SentEval.senteval as senteval\n",
    "#import SentEval_simcse.senteval as senteval\n",
    "#import SentEval_simcse.senteval.engine as se_engine\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s) for s in batch]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model2(**batch)\n",
    "\n",
    "    pooled_result = outputs.pooler_output.cpu()\n",
    "\n",
    "    return pooled_result\n",
    "\n",
    "\n",
    "def evaluate_model():\n",
    "    PATH_TO_DATA = \"./data\"\n",
    "\n",
    "    params = {\"task_path\": PATH_TO_DATA, \"usepytorch\": True, \"kfold\": 10}\n",
    "    tasks = [\"STSBenchmark\", 'STS12', 'STS13', 'STS14', 'STS15']\n",
    "\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "    #se = se_engine.SE(params, batcher, prepare)\n",
    "    results = se.eval(tasks)\n",
    "\n",
    "    print('STS12: ', results[\"STS12\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS13: ', results[\"STS13\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS14: ', results[\"STS14\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS15: ', results[\"STS15\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STSB: ', results[\"STSBenchmark\"][\"test\"][\"spearman\"][0])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS12:  0.5018383531025148\n",
      "STS13:  0.6031743482177943\n",
      "STS14:  0.5420846725534382\n",
      "STS15:  0.709963833540791\n",
      "STSB:  0.6169321318040776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.6391098590596108, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.626583628661332, pvalue=0.0),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.6898940758840367, 1.9177910221092845e-212),\n",
       "   'spearman': SpearmanrResult(correlation=0.699399105438588, pvalue=9.441469501593988e-221),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.5972605440525323, 4.3553688664292627e-134),\n",
       "   'spearman': SpearmanrResult(correlation=0.6169321318040776, pvalue=2.0477681368590674e-145),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.6424793053926677,\n",
       "    'mean': 0.6420881596653932,\n",
       "    'wmean': 0.641250114025058},\n",
       "   'spearman': {'all': 0.6412911964593353,\n",
       "    'mean': 0.6476382886346658,\n",
       "    'wmean': 0.6377002027224968}}},\n",
       " 'STS12': {'MSRpar': {'pearson': (0.487659215981259, 4.6368531953735704e-46),\n",
       "   'spearman': SpearmanrResult(correlation=0.5018279806741894, pvalue=4.349175756398707e-49),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (0.6963944884125152, 7.050639813207183e-110),\n",
       "   'spearman': SpearmanrResult(correlation=0.6971979109166193, pvalue=3.120649284927418e-110),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.4821048249300228, 4.321473500958679e-28),\n",
       "   'spearman': SpearmanrResult(correlation=0.5826248585597907, pvalue=4.494958362540392e-43),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.6913024332100217, 1.1607032483564747e-107),\n",
       "   'spearman': SpearmanrResult(correlation=0.6925913305463709, pvalue=3.221489357356335e-108),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.5905628855165638, 7.177929825841345e-39),\n",
       "   'spearman': SpearmanrResult(correlation=0.5031164265435667, pvalue=5.457948466767591e-27),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.4858586359369039,\n",
       "    'mean': 0.5896047696100765,\n",
       "    'wmean': 0.5995617146611443},\n",
       "   'spearman': {'all': 0.5018383531025148,\n",
       "    'mean': 0.5954717014481073,\n",
       "    'wmean': 0.607104627050422}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.3610807040900506, 3.318964279360886e-07),\n",
       "   'spearman': SpearmanrResult(correlation=0.3741009552679192, pvalue=1.1408009585323912e-07),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (0.6985891798487365, 7.559040763006796e-111),\n",
       "   'spearman': SpearmanrResult(correlation=0.6782795355301314, pvalue=3.380922261300474e-102),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.5081888154034115, 3.596207157445632e-38),\n",
       "   'spearman': SpearmanrResult(correlation=0.5472597322050619, pvalue=3.808947545100601e-45),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': 0.5917274354615059,\n",
       "    'mean': 0.5226195664473995,\n",
       "    'wmean': 0.5848533756005905},\n",
       "   'spearman': {'all': 0.6031743482177943,\n",
       "    'mean': 0.5332134076677041,\n",
       "    'wmean': 0.5909516279735166}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (0.3878627837346996,\n",
       "    1.3238576936422252e-17),\n",
       "   'spearman': SpearmanrResult(correlation=0.4048653341473826, pvalue=3.531381114127859e-19),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (0.7444398530831836, 3.2711564096031405e-54),\n",
       "   'spearman': SpearmanrResult(correlation=0.6983174384886176, pvalue=3.505088702902871e-45),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (0.6682186321350516, 3.6346786361101796e-98),\n",
       "   'spearman': SpearmanrResult(correlation=0.6345598122596934, pvalue=9.262335413977888e-86),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.7052281144452922, 7.769026338877664e-114),\n",
       "   'spearman': SpearmanrResult(correlation=0.6912930668204175, pvalue=1.1715362666907267e-107),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.6423180881269219, 1.758589545649314e-88),\n",
       "   'spearman': SpearmanrResult(correlation=0.684768798129304, pvalue=6.9509307920934584e-105),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (0.6729503673082559, 4.834353512049406e-100),\n",
       "   'spearman': SpearmanrResult(correlation=0.6501249811342555, pvalue=2.6656020839925707e-91),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.5474688806929956,\n",
       "    'mean': 0.6368363064722341,\n",
       "    'wmean': 0.6438417626979229},\n",
       "   'spearman': {'all': 0.5420846725534382,\n",
       "    'mean': 0.6273215718299451,\n",
       "    'wmean': 0.6365985668455094}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.6060633974296676,\n",
       "    5.6631244696555336e-39),\n",
       "   'spearman': SpearmanrResult(correlation=0.6052451345419675, pvalue=7.593177956469514e-39),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.7208455337594114,\n",
       "    3.3098463338987895e-121),\n",
       "   'spearman': SpearmanrResult(correlation=0.7267735725456143, pvalue=3.87665844747485e-124),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.6617938311863294, 1.3291976255031946e-48),\n",
       "   'spearman': SpearmanrResult(correlation=0.6660875743364179, pvalue=1.9721667001558314e-49),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.7362868517603849, 5.2363488085831693e-129),\n",
       "   'spearman': SpearmanrResult(correlation=0.7320074954339812, pvalue=8.61611492040793e-127),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.7715485612644942, 4.467357956570216e-149),\n",
       "   'spearman': SpearmanrResult(correlation=0.7784550712225645, pvalue=1.9588310635781975e-153),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.7024066821483996,\n",
       "    'mean': 0.6993076350800576,\n",
       "    'wmean': 0.7156523902730721},\n",
       "   'spearman': {'all': 0.709963833540791,\n",
       "    'mean': 0.7017137696161091,\n",
       "    'wmean': 0.7182256234103382}}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.eval()\n",
    "\n",
    "results2 = evaluate_model()\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.05 temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertCLModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertCLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertCLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertCLModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertCLModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from transformers import AutoTokenizer, DistilBertModel, DistilBertPreTrainedModel, AutoConfig\n",
    "# from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPooling\n",
    "\n",
    "# from src.distilface.modules.pooler import Pooler\n",
    "# from src.distilface.modules.similarity import Similarity\n",
    "\n",
    "#from torch.cuda.amp import autocast \n",
    "\n",
    "#scaler = GradScaler()\n",
    "\n",
    "class DistilBertCLModel(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, pooler_type='avg_first_last', temp=0.05):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.pooler_type = pooler_type\n",
    "        self.temp = 0.05\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pooler = Pooler(pooler_type)\n",
    "        self.sim = Similarity(temp=temp)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        if self.training:\n",
    "            return self.cl_forward(self.distilbert, input_ids, attention_mask)\n",
    "        else:\n",
    "            return self.sent_emb(self.distilbert, input_ids, attention_mask)\n",
    "\n",
    "    def cl_forward(self, encoder, input_ids=None, attention_mask=None):\n",
    "        batch_size = input_ids.size(0)#64#input_ids.size(0)\n",
    "        num_sent = input_ids.size(1)  # Number of sentences in one instance: 2 sentences\n",
    "\n",
    "        # Flatten all input tensors\n",
    "        input_ids = input_ids.view((-1, input_ids.size(-1))) # (bs * num_sent, len)\n",
    "        attention_mask = attention_mask.view((-1, attention_mask.size(-1))) # (bs * num_sent len)\n",
    "\n",
    "        # Pre-trained Model Encoder\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # Pooling\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "        pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-1)))  # (bs, num_sent, hidden)\n",
    "\n",
    "        # Separate representation\n",
    "        z1, z2 = pooler_output[:, 0], pooler_output[:, 1]\n",
    "\n",
    "        # Cosine similarity\n",
    "        cos_sim = self.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "\n",
    "        # Calculate contrastive loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(self.device)\n",
    "        loss = criterion(cos_sim, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cos_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def sent_emb(self, encoder, input_ids=None, attention_mask=None):\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            pooler_output=pooler_output,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )\n",
    "\n",
    "\n",
    "pretrained_model_name = 'distilbert-base-uncased'\n",
    "\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model3 = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertCLModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertCLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertCLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertCLModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertCLModel for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# import mlflow\n",
    "\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "# from transformers import default_data_collator\n",
    "\n",
    "#model3 = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "\n",
    "training_args3 = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=False,\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size= 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    weight_decay=0.0,\n",
    "    num_train_epochs=2,\n",
    "    max_steps= 20000,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "model3.train()\n",
    "\n",
    "trainer3 = Trainer(\n",
    "    model=model3,\n",
    "    args=training_args3,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\anaconda3\\envs\\cl-distilled\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20000/20000 32:23, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output\\checkpoint-5000\n",
      "Configuration saved in output\\checkpoint-5000\\config.json\n",
      "Model weights saved in output\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-10000\n",
      "Configuration saved in output\\checkpoint-10000\\config.json\n",
      "Model weights saved in output\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-15000\n",
      "Configuration saved in output\\checkpoint-15000\\config.json\n",
      "Model weights saved in output\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-15000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-20000\n",
      "Configuration saved in output\\checkpoint-20000\\config.json\n",
      "Model weights saved in output\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-20000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'trained_model/distilbert_cl'\n",
    "\n",
    "#train_result = trainer.train(model_path=model_path)\n",
    "train_result3 = trainer3.train()\n",
    "torch.save(model3, './0_05temp_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import senteval\n",
    "#import SentEval.senteval as senteval\n",
    "#import SentEval_simcse.senteval as senteval\n",
    "#import SentEval_simcse.senteval.engine as se_engine\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s) for s in batch]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model3(**batch)\n",
    "\n",
    "    pooled_result = outputs.pooler_output.cpu()\n",
    "\n",
    "    return pooled_result\n",
    "\n",
    "\n",
    "def evaluate_model():\n",
    "    PATH_TO_DATA = \"./data\"\n",
    "\n",
    "    params = {\"task_path\": PATH_TO_DATA, \"usepytorch\": True, \"kfold\": 10}\n",
    "    tasks = [\"STSBenchmark\", 'STS12', 'STS13', 'STS14', 'STS15']\n",
    "\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "    #se = se_engine.SE(params, batcher, prepare)\n",
    "    results = se.eval(tasks)\n",
    "\n",
    "    print('STS12: ', results[\"STS12\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS13: ', results[\"STS13\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS14: ', results[\"STS14\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS15: ', results[\"STS15\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STSB: ', results[\"STSBenchmark\"][\"test\"][\"spearman\"][0])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS12:  0.6056512503604072\n",
      "STS13:  0.7664013170536642\n",
      "STS14:  0.6853466562577095\n",
      "STS15:  0.7770547451580875\n",
      "STSB:  0.7409549122778031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.7636742615728462, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.7448408409102731, pvalue=0.0),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.7790914556746467, 4.3252980564806194e-306),\n",
       "   'spearman': SpearmanrResult(correlation=0.7829357887267215, pvalue=4.2210875529375e-311),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.7422504324127708, 1.1912635357145677e-241),\n",
       "   'spearman': SpearmanrResult(correlation=0.7409549122778031, pvalue=2.2567862805118097e-240),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.7610125124249868,\n",
       "    'mean': 0.7616720498867545,\n",
       "    'wmean': 0.7629304426972037},\n",
       "   'spearman': {'all': 0.7549624254583777,\n",
       "    'mean': 0.7562438473049325,\n",
       "    'wmean': 0.7508426635969324}}},\n",
       " 'STS12': {'MSRpar': {'pearson': (0.6241236099371741, 3.214061362436737e-82),\n",
       "   'spearman': SpearmanrResult(correlation=0.6155870786598757, pvalue=2.0165685333515305e-79),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (0.8292242043157813, 3.640722007450388e-191),\n",
       "   'spearman': SpearmanrResult(correlation=0.8253438108670114, pvalue=7.37320911919696e-188),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.5041270107678866, 5.863128542098031e-31),\n",
       "   'spearman': SpearmanrResult(correlation=0.6062586977770228, pvalue=2.0990394835589054e-47),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.7094379895069458, 8.950278546276737e-116),\n",
       "   'spearman': SpearmanrResult(correlation=0.6894973561962484, pvalue=6.90890340946309e-107),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.6008061093601155, 1.6514702983299795e-40),\n",
       "   'spearman': SpearmanrResult(correlation=0.5513907561773156, pvalue=4.095945641656507e-33),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.6366766782590316,\n",
       "    'mean': 0.6535437847775806,\n",
       "    'wmean': 0.6734894750312329},\n",
       "   'spearman': {'all': 0.6056512503604072,\n",
       "    'mean': 0.6576155399354947,\n",
       "    'wmean': 0.6744204756392386}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.5880666111690176, 5.7453536296589085e-19),\n",
       "   'spearman': SpearmanrResult(correlation=0.6033830333267636, pvalue=3.977600034883091e-20),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (0.7803382586140274, 1.1920329302294101e-154),\n",
       "   'spearman': SpearmanrResult(correlation=0.7816645834341912, pvalue=1.6323252182342097e-155),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.7727184051496132, 1.9679859331627785e-112),\n",
       "   'spearman': SpearmanrResult(correlation=0.769541660113129, pvalue=5.793640042819125e-111),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': 0.7582299805249222,\n",
       "    'mean': 0.7137077583108861,\n",
       "    'wmean': 0.7532622058402652},\n",
       "   'spearman': {'all': 0.7664013170536642,\n",
       "    'mean': 0.7181964256246945,\n",
       "    'wmean': 0.7546671347985782}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (0.5232787909081715,\n",
       "    5.35339488668378e-33),\n",
       "   'spearman': SpearmanrResult(correlation=0.5037409924997994, pvalue=2.4917292946188965e-30),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (0.8037665983892481, 3.5693216018493976e-69),\n",
       "   'spearman': SpearmanrResult(correlation=0.7725005734714443, pvalue=1.01165603080223e-60),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (0.7652106059708903, 3.283260900576635e-145),\n",
       "   'spearman': SpearmanrResult(correlation=0.7472111117543135, pvalue=7.237717691302586e-135),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.7720136714083515, 2.298285864787046e-149),\n",
       "   'spearman': SpearmanrResult(correlation=0.7463776969185124, pvalue=2.07590517719209e-134),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.8145546661344735, 4.429780721107421e-179),\n",
       "   'spearman': SpearmanrResult(correlation=0.81781142621003, pvalue=1.1430667589981953e-181),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (0.748003812753413, 2.6465420781099702e-135),\n",
       "   'spearman': SpearmanrResult(correlation=0.6981022801339534, pvalue=1.2427210184006659e-110),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.7082895102115839,\n",
       "    'mean': 0.7378046909274247,\n",
       "    'wmean': 0.7470513340335461},\n",
       "   'spearman': {'all': 0.6853466562577095,\n",
       "    'mean': 0.7142906801646755,\n",
       "    'wmean': 0.7241494679810533}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.7480570249326417,\n",
       "    2.0914236619397645e-68),\n",
       "   'spearman': SpearmanrResult(correlation=0.7431664215104349, pvalue=4.503308779417233e-67),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.7032571302296753, 6.111815068441861e-113),\n",
       "   'spearman': SpearmanrResult(correlation=0.704232490419828, pvalue=2.2069964425692468e-113),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.7828566406263944, 7.208231237121476e-79),\n",
       "   'spearman': SpearmanrResult(correlation=0.7899403200808167, pvalue=3.081828216392254e-81),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.8010870722743396, 6.773115636810957e-169),\n",
       "   'spearman': SpearmanrResult(correlation=0.8035016953928569, pvalue=1.1569794052002857e-170),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.8498699966335049, 2.7856344507925724e-210),\n",
       "   'spearman': SpearmanrResult(correlation=0.8553631467748758, pvalue=7.400013039368519e-216),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.7751446209064347,\n",
       "    'mean': 0.7770255729393112,\n",
       "    'wmean': 0.7799177579792594},\n",
       "   'spearman': {'all': 0.7770547451580875,\n",
       "    'mean': 0.7792408148357625,\n",
       "    'wmean': 0.7824126758457967}}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.eval()\n",
    "\n",
    "results3 = evaluate_model()\n",
    "results3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertCLModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertCLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertCLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertCLModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertCLModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DistilBertCLModel(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, pooler_type='avg_first_last', temp=0.1):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.pooler_type = pooler_type\n",
    "        self.temp = 0.1\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pooler = Pooler(pooler_type)\n",
    "        self.sim = Similarity(temp=temp)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        if self.training:\n",
    "            return self.cl_forward(self.distilbert, input_ids, attention_mask)\n",
    "        else:\n",
    "            return self.sent_emb(self.distilbert, input_ids, attention_mask)\n",
    "\n",
    "    def cl_forward(self, encoder, input_ids=None, attention_mask=None):\n",
    "        batch_size = input_ids.size(0)#64#input_ids.size(0)\n",
    "        num_sent = input_ids.size(1)  # Number of sentences in one instance: 2 sentences\n",
    "\n",
    "        # Flatten all input tensors\n",
    "        input_ids = input_ids.view((-1, input_ids.size(-1))) # (bs * num_sent, len)\n",
    "        attention_mask = attention_mask.view((-1, attention_mask.size(-1))) # (bs * num_sent len)\n",
    "\n",
    "        # Pre-trained Model Encoder\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # Pooling\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "        pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-1)))  # (bs, num_sent, hidden)\n",
    "\n",
    "        # Separate representation\n",
    "        z1, z2 = pooler_output[:, 0], pooler_output[:, 1]\n",
    "\n",
    "        # Cosine similarity\n",
    "        cos_sim = self.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "\n",
    "        # Calculate contrastive loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(self.device)\n",
    "        loss = criterion(cos_sim, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cos_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def sent_emb(self, encoder, input_ids=None, attention_mask=None):\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            pooler_output=pooler_output,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )\n",
    "    \n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model4 = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# import mlflow\n",
    "\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "# from transformers import default_data_collator\n",
    "\n",
    "#model3 = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "\n",
    "training_args4 = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=False,\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size= 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    weight_decay=0.0,\n",
    "    num_train_epochs=2,\n",
    "    max_steps= 20000,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "model4.train()\n",
    "\n",
    "trainer4 = Trainer(\n",
    "    model=model4,\n",
    "    args=training_args4,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\anaconda3\\envs\\cl-distilled\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20000/20000 32:19, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output\\checkpoint-5000\n",
      "Configuration saved in output\\checkpoint-5000\\config.json\n",
      "Model weights saved in output\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-10000\n",
      "Configuration saved in output\\checkpoint-10000\\config.json\n",
      "Model weights saved in output\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-15000\n",
      "Configuration saved in output\\checkpoint-15000\\config.json\n",
      "Model weights saved in output\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-15000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-20000\n",
      "Configuration saved in output\\checkpoint-20000\\config.json\n",
      "Model weights saved in output\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-20000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'trained_model/distilbert_cl'\n",
    "\n",
    "#train_result = trainer.train(model_path=model_path)\n",
    "train_result4 = trainer4.train()\n",
    "torch.save(model4, './0_1temp_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import senteval\n",
    "#import SentEval.senteval as senteval\n",
    "#import SentEval_simcse.senteval as senteval\n",
    "#import SentEval_simcse.senteval.engine as se_engine\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s) for s in batch]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model4(**batch)\n",
    "\n",
    "    pooled_result = outputs.pooler_output.cpu()\n",
    "\n",
    "    return pooled_result\n",
    "\n",
    "\n",
    "def evaluate_model():\n",
    "    PATH_TO_DATA = \"./data\"\n",
    "\n",
    "    params = {\"task_path\": PATH_TO_DATA, \"usepytorch\": True, \"kfold\": 10}\n",
    "    tasks = [\"STSBenchmark\", 'STS12', 'STS13', 'STS14', 'STS15']\n",
    "\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "    #se = se_engine.SE(params, batcher, prepare)\n",
    "    results = se.eval(tasks)\n",
    "\n",
    "    print('STS12: ', results[\"STS12\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS13: ', results[\"STS13\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS14: ', results[\"STS14\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS15: ', results[\"STS15\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STSB: ', results[\"STSBenchmark\"][\"test\"][\"spearman\"][0])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS12:  0.5038198940159697\n",
      "STS13:  0.6404188064258012\n",
      "STS14:  0.5539911624009456\n",
      "STS15:  0.6540062512499364\n",
      "STSB:  0.5631252826099555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.6219086558842281, 0.0),\n",
       "   'spearman': SpearmanrResult(correlation=0.5975192167388444, pvalue=0.0),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.6220616320913329, 2.2314321216343802e-161),\n",
       "   'spearman': SpearmanrResult(correlation=0.6349512169816413, pvalue=4.235121978076463e-170),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.5617961659198661, 1.5365510632076718e-115),\n",
       "   'spearman': SpearmanrResult(correlation=0.5631252826099555, pvalue=3.400716345614139e-116),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.6104371686656989,\n",
       "    'mean': 0.6019221512984757,\n",
       "    'wmean': 0.6123275641653827},\n",
       "   'spearman': {'all': 0.6049080443331466,\n",
       "    'mean': 0.5985319054434804,\n",
       "    'wmean': 0.5985297365812711}}},\n",
       " 'STS12': {'MSRpar': {'pearson': (0.47395765375148974, 2.8991109682396544e-43),\n",
       "   'spearman': SpearmanrResult(correlation=0.4672447933970129, pvalue=6.11751567793586e-42),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (0.6692804822163766, 1.3882386105753449e-98),\n",
       "   'spearman': SpearmanrResult(correlation=0.6675425085398634, pvalue=6.694585074807118e-98),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.3760376616524345, 7.272155059846777e-17),\n",
       "   'spearman': SpearmanrResult(correlation=0.48609804181905625, pvalue=1.352077334081643e-28),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.6841252580157635, 1.2931444950867263e-104),\n",
       "   'spearman': SpearmanrResult(correlation=0.6875841944924768, pvalue=4.511218187229109e-106),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.5471616743910793, 1.5459704695826572e-32),\n",
       "   'spearman': SpearmanrResult(correlation=0.49975979194761005, pvalue=1.341403756114539e-26),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.49113674300920107,\n",
       "    'mean': 0.5501125460054287,\n",
       "    'wmean': 0.5667443179756211},\n",
       "   'spearman': {'all': 0.5038198940159697,\n",
       "    'mean': 0.5616458660392039,\n",
       "    'wmean': 0.5757084235856043}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.42214908099866105, 1.4457642170546697e-09),\n",
       "   'spearman': SpearmanrResult(correlation=0.44785435339401575, pvalue=1.0360244807348751e-10),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (0.6894926774797134, 6.94080265556276e-107),\n",
       "   'spearman': SpearmanrResult(correlation=0.6718356022334848, pvalue=1.3473555743616226e-99),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.5884243091309556, 1.4646672112159127e-53),\n",
       "   'spearman': SpearmanrResult(correlation=0.6090187599279125, pvalue=3.0444939655951904e-58),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': 0.6223929984568705,\n",
       "    'mean': 0.56668868920311,\n",
       "    'wmean': 0.6180078145606653},\n",
       "   'spearman': {'all': 0.6404188064258012,\n",
       "    'mean': 0.5762362385184711,\n",
       "    'wmean': 0.6201204658574276}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (0.38622902404255793,\n",
       "    1.8549488484543585e-17),\n",
       "   'spearman': SpearmanrResult(correlation=0.40211279265954314, pvalue=6.440785490716368e-19),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (0.7018434612518526, 8.240943019404461e-46),\n",
       "   'spearman': SpearmanrResult(correlation=0.6956928576999015, pvalue=1.0156899111737977e-44),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (0.6544529269908489, 6.708835512221485e-93),\n",
       "   'spearman': SpearmanrResult(correlation=0.6086599902381062, pvalue=3.253975746420163e-77),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.6781054012432834, 3.9827509584176044e-102),\n",
       "   'spearman': SpearmanrResult(correlation=0.6468178762830543, pvalue=4.2679945097856195e-90),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.695433005967504, 1.8634198297314675e-109),\n",
       "   'spearman': SpearmanrResult(correlation=0.7206672370818982, pvalue=4.043838109648309e-121),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (0.6759621061402303, 2.963975391477351e-101),\n",
       "   'spearman': SpearmanrResult(correlation=0.6277890095935857, pvalue=1.9010633240425636e-83),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.5758294005763502,\n",
       "    'mean': 0.6320043209393794,\n",
       "    'wmean': 0.6432856478536284},\n",
       "   'spearman': {'all': 0.5539911624009456,\n",
       "    'mean': 0.6169566272593482,\n",
       "    'wmean': 0.6246957863744661}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.5237393793815552,\n",
       "    8.40510109342278e-28),\n",
       "   'spearman': SpearmanrResult(correlation=0.5067165256095083, pvalue=7.466775780255818e-26),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.6585823101809735, 1.8884322580632816e-94),\n",
       "   'spearman': SpearmanrResult(correlation=0.6543280147978029, pvalue=7.467423651525283e-93),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.6616947967570543, 1.3884832823952923e-48),\n",
       "   'spearman': SpearmanrResult(correlation=0.6696024768809297, pvalue=4.038904536399163e-50),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.7366384864718994, 3.427601510205504e-129),\n",
       "   'spearman': SpearmanrResult(correlation=0.7295557450888793, pvalue=1.534219620472827e-125),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.7413549454277918, 1.0903329726394886e-131),\n",
       "   'spearman': SpearmanrResult(correlation=0.747256321267598, pvalue=6.83481683884154e-135),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.6528235576752351,\n",
       "    'mean': 0.6644019836438548,\n",
       "    'wmean': 0.6823232075374924},\n",
       "   'spearman': {'all': 0.6540062512499364,\n",
       "    'mean': 0.6614918167289436,\n",
       "    'wmean': 0.6798248955998748}}}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.eval()\n",
    "\n",
    "results4 = evaluate_model()\n",
    "results4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertCLModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertCLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertCLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertCLModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertCLModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ng-ka/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DistilBertCLModel(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, pooler_type='avg_first_last', temp=1):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.pooler_type = pooler_type\n",
    "        self.temp = 1\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pooler = Pooler(pooler_type)\n",
    "        self.sim = Similarity(temp=temp)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        if self.training:\n",
    "            return self.cl_forward(self.distilbert, input_ids, attention_mask)\n",
    "        else:\n",
    "            return self.sent_emb(self.distilbert, input_ids, attention_mask)\n",
    "\n",
    "    def cl_forward(self, encoder, input_ids=None, attention_mask=None):\n",
    "        batch_size = input_ids.size(0)#64#input_ids.size(0)\n",
    "        num_sent = input_ids.size(1)  # Number of sentences in one instance: 2 sentences\n",
    "\n",
    "        # Flatten all input tensors\n",
    "        input_ids = input_ids.view((-1, input_ids.size(-1))) # (bs * num_sent, len)\n",
    "        attention_mask = attention_mask.view((-1, attention_mask.size(-1))) # (bs * num_sent len)\n",
    "\n",
    "        # Pre-trained Model Encoder\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # Pooling\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "        pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-1)))  # (bs, num_sent, hidden)\n",
    "\n",
    "        # Separate representation\n",
    "        z1, z2 = pooler_output[:, 0], pooler_output[:, 1]\n",
    "\n",
    "        # Cosine similarity\n",
    "        cos_sim = self.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "\n",
    "        # Calculate contrastive loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(self.device)\n",
    "        loss = criterion(cos_sim, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cos_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def sent_emb(self, encoder, input_ids=None, attention_mask=None):\n",
    "        outputs = encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        pooler_output = self.pooler(attention_mask, outputs)\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            pooler_output=pooler_output,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )\n",
    "    \n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model5 = DistilBertCLModel.from_pretrained(pretrained_model_name, config=config).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# import mlflow\n",
    "\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "# from transformers import default_data_collator\n",
    "\n",
    "training_args5 = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir=False,\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size= 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    weight_decay=0.0,\n",
    "    num_train_epochs=2,\n",
    "    max_steps= 20000,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "model5.train()\n",
    "\n",
    "trainer5 = Trainer(\n",
    "    model=model5,\n",
    "    args=training_args4,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\anaconda3\\envs\\cl-distilled\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20000/20000 31:52, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.196100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output\\checkpoint-5000\n",
      "Configuration saved in output\\checkpoint-5000\\config.json\n",
      "Model weights saved in output\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-10000\n",
      "Configuration saved in output\\checkpoint-10000\\config.json\n",
      "Model weights saved in output\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-15000\n",
      "Configuration saved in output\\checkpoint-15000\\config.json\n",
      "Model weights saved in output\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-15000\\special_tokens_map.json\n",
      "Saving model checkpoint to output\\checkpoint-20000\n",
      "Configuration saved in output\\checkpoint-20000\\config.json\n",
      "Model weights saved in output\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in output\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in output\\checkpoint-20000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'trained_model/distilbert_cl'\n",
    "\n",
    "#train_result = trainer.train(model_path=model_path)\n",
    "train_result5 = trainer5.train()\n",
    "torch.save(model5, './1temp_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import senteval\n",
    "#import SentEval.senteval as senteval\n",
    "#import SentEval_simcse.senteval as senteval\n",
    "#import SentEval_simcse.senteval.engine as se_engine\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s) for s in batch]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model5(**batch)\n",
    "\n",
    "    pooled_result = outputs.pooler_output.cpu()\n",
    "\n",
    "    return pooled_result\n",
    "\n",
    "\n",
    "def evaluate_model():\n",
    "    PATH_TO_DATA = \"./data\"\n",
    "\n",
    "    params = {\"task_path\": PATH_TO_DATA, \"usepytorch\": True, \"kfold\": 10}\n",
    "    tasks = [\"STSBenchmark\", 'STS12', 'STS13', 'STS14', 'STS15']\n",
    "\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "    #se = se_engine.SE(params, batcher, prepare)\n",
    "    results = se.eval(tasks)\n",
    "\n",
    "    print('STS12: ', results[\"STS12\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS13: ', results[\"STS13\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS14: ', results[\"STS14\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STS15: ', results[\"STS15\"][\"all\"][\"spearman\"][\"all\"])\n",
    "    print('STSB: ', results[\"STSBenchmark\"][\"test\"][\"spearman\"][0])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "C:\\Users\\ng-ka\\OMSCS\\DL\\DLProject\\contrastive-learning-in-distilled-models\\senteval\\sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS12:  0.43052863516877926\n",
      "STS13:  0.4545311337853271\n",
      "STS14:  0.3925715299069684\n",
      "STS15:  0.4786150299588344\n",
      "STSB:  0.3236416143176774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'STSBenchmark': {'train': {'pearson': (0.4615603362668978,\n",
       "    2.403907071103749e-301),\n",
       "   'spearman': SpearmanrResult(correlation=0.44040623851761684, pvalue=1.9830712984173826e-271),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (0.4775079329207626, 2.8424217770169393e-86),\n",
       "   'spearman': SpearmanrResult(correlation=0.5119757905872877, pvalue=5.3176257500031175e-101),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.3237255679553281, 5.213444696553968e-35),\n",
       "   'spearman': SpearmanrResult(correlation=0.3236416143176774, pvalue=5.437368047298008e-35),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.44654659850642947,\n",
       "    'mean': 0.42093127904766287,\n",
       "    'wmean': 0.4423029474721763},\n",
       "   'spearman': {'all': 0.4400559581707548,\n",
       "    'mean': 0.425341214474194,\n",
       "    'wmean': 0.4341864785886402}}},\n",
       " 'STS12': {'MSRpar': {'pearson': (0.2238989258812755, 5.632657830685157e-10),\n",
       "   'spearman': SpearmanrResult(correlation=0.24478479926802538, pvalue=1.0742750796582411e-11),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (0.5264989713698751, 1.0338871993436564e-54),\n",
       "   'spearman': SpearmanrResult(correlation=0.5377779469876163, pvalue=1.9283865212392557e-57),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.3669401846024595, 4.468425453732613e-16),\n",
       "   'spearman': SpearmanrResult(correlation=0.4824578812407137, pvalue=3.901956250657792e-28),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.5623001896596467, 9.583144243759109e-64),\n",
       "   'spearman': SpearmanrResult(correlation=0.6160742652263818, pvalue=1.4036634022015676e-79),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.4643930928322424, 9.784198694047359e-23),\n",
       "   'spearman': SpearmanrResult(correlation=0.5156148813570837, pvalue=1.7548959562795311e-28),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.3771940129985165,\n",
       "    'mean': 0.42880627286909984,\n",
       "    'wmean': 0.430579779265023},\n",
       "   'spearman': {'all': 0.43052863516877926,\n",
       "    'mean': 0.47934195481596414,\n",
       "    'wmean': 0.4749537528193313}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.2968648869825213, 3.353311868840129e-05),\n",
       "   'spearman': SpearmanrResult(correlation=0.3154843634204353, pvalue=9.784825408912543e-06),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (0.4687654733565954, 3.084238630262968e-42),\n",
       "   'spearman': SpearmanrResult(correlation=0.4533492925398607, pvalue=2.729544448306387e-39),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.35258641989494693, 7.268092696948227e-18),\n",
       "   'spearman': SpearmanrResult(correlation=0.39390095672433967, pvalue=2.8992876618895965e-22),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': 0.4429329179877229,\n",
       "    'mean': 0.3727389267446879,\n",
       "    'wmean': 0.40365503347880555},\n",
       "   'spearman': {'all': 0.4545311337853271,\n",
       "    'mean': 0.3875782042282119,\n",
       "    'wmean': 0.41374463387580823}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (0.21882226415758907,\n",
       "    2.7870596271601288e-06),\n",
       "   'spearman': SpearmanrResult(correlation=0.1997580898769047, pvalue=1.9650747010705744e-05),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (0.5129081470852415, 1.5723651035113123e-21),\n",
       "   'spearman': SpearmanrResult(correlation=0.5085701742201417, pvalue=3.873365751759426e-21),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (0.47026184039184066, 1.5667920662417296e-42),\n",
       "   'spearman': SpearmanrResult(correlation=0.4318223931835671, pvalue=2.0260437578517114e-35),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.6186875753520803, 1.9883648673841077e-80),\n",
       "   'spearman': SpearmanrResult(correlation=0.5875623257617801, pvalue=8.166334046732035e-71),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.4701139447995143, 1.6755125511878385e-42),\n",
       "   'spearman': SpearmanrResult(correlation=0.5228447475257707, pvalue=7.531511736581148e-54),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (0.527134595330579, 7.301081258034792e-55),\n",
       "   'spearman': SpearmanrResult(correlation=0.47659023355308244, pvalue=8.606542951396187e-44),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.412060750627634,\n",
       "    'mean': 0.46965472785280743,\n",
       "    'wmean': 0.4845309146405329},\n",
       "   'spearman': {'all': 0.3925715299069684,\n",
       "    'mean': 0.45452466068687447,\n",
       "    'wmean': 0.46842052472768}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.22691050842694566,\n",
       "    9.103168489385915e-06),\n",
       "   'spearman': SpearmanrResult(correlation=0.2194068250581788, pvalue=1.8113595160583808e-05),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.47756888690191873, 5.46489807534883e-44),\n",
       "   'spearman': SpearmanrResult(correlation=0.4739404543106108, pvalue=2.9221039379423315e-43),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.3537894279647163, 1.6917855611606995e-12),\n",
       "   'spearman': SpearmanrResult(correlation=0.3703797843594116, pvalue=1.227866352597013e-13),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.5205891346660505, 2.535091743152569e-53),\n",
       "   'spearman': SpearmanrResult(correlation=0.5389904684030932, pvalue=9.673916940692206e-58),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.6040680804562857, 8.833949882262205e-76),\n",
       "   'spearman': SpearmanrResult(correlation=0.6227742113005392, pvalue=9.016688119988997e-82),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.4659567571240947,\n",
       "    'mean': 0.4365852076831834,\n",
       "    'wmean': 0.4731440175550214},\n",
       "   'spearman': {'all': 0.4786150299588344,\n",
       "    'mean': 0.44509834868636666,\n",
       "    'wmean': 0.4826496096807596}}}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.eval()\n",
    "\n",
    "results5 = evaluate_model()\n",
    "results5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl-distilled",
   "language": "python",
   "name": "cl-distilled"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
